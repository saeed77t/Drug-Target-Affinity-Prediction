{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6bad0f-0d46-4371-9b46-939bccfc6960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of drugs = 2068\n",
      "number of proteins =  229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drug_ID</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Target_ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O00141</td>\n",
       "      <td>MTVKTEAAKGTLTYSRMRGMVAILIAFMKQRRMGLNDFIQKIANNS...</td>\n",
       "      <td>11.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O14920</td>\n",
       "      <td>MSWSPSLTTQTCGAWEMKERLGTGGFGNVIRWHNQETGEQIAIKQC...</td>\n",
       "      <td>11.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>O15111</td>\n",
       "      <td>MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...</td>\n",
       "      <td>11.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>P00533</td>\n",
       "      <td>MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...</td>\n",
       "      <td>11.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL1087421</td>\n",
       "      <td>COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2</td>\n",
       "      <td>P04626</td>\n",
       "      <td>MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...</td>\n",
       "      <td>11.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117652</th>\n",
       "      <td>CHEMBL230654</td>\n",
       "      <td>CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...</td>\n",
       "      <td>Q13554</td>\n",
       "      <td>MATTVTCTRFTDEYQLYEDIGKGAFSVVRRCVKLCTGHEYAAKIIN...</td>\n",
       "      <td>10.49794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117653</th>\n",
       "      <td>CHEMBL230654</td>\n",
       "      <td>CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...</td>\n",
       "      <td>Q13555</td>\n",
       "      <td>MATTATCTRFTDDYQLFEELGKGAFSVVRRCVKKTSTQEYAAKIIN...</td>\n",
       "      <td>10.49794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117654</th>\n",
       "      <td>CHEMBL230654</td>\n",
       "      <td>CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...</td>\n",
       "      <td>Q13557</td>\n",
       "      <td>MASTTTCTRFTDEYQLFEELGKGAFSVVRRCMKIPTGQEYAAKIIN...</td>\n",
       "      <td>10.49794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117655</th>\n",
       "      <td>CHEMBL230654</td>\n",
       "      <td>CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...</td>\n",
       "      <td>Q16539</td>\n",
       "      <td>MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKT...</td>\n",
       "      <td>10.49794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117656</th>\n",
       "      <td>CHEMBL230654</td>\n",
       "      <td>CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...</td>\n",
       "      <td>Q9UQM7</td>\n",
       "      <td>MATITCTRFTEEYQLFEELGKGAFSVVRRCVKVLAGQEYAAKIINT...</td>\n",
       "      <td>10.49794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117657 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Drug_ID                                               Drug  \\\n",
       "0       CHEMBL1087421            COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2   \n",
       "1       CHEMBL1087421            COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2   \n",
       "2       CHEMBL1087421            COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2   \n",
       "3       CHEMBL1087421            COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2   \n",
       "4       CHEMBL1087421            COc1cc2c(cc1Cl)C(c1ccc(Cl)c(Cl)c1)=NCC2   \n",
       "...               ...                                                ...   \n",
       "117652   CHEMBL230654  CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...   \n",
       "117653   CHEMBL230654  CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...   \n",
       "117654   CHEMBL230654  CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...   \n",
       "117655   CHEMBL230654  CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...   \n",
       "117656   CHEMBL230654  CCCc1nc[nH]c1CNc1cc(Cl)c2ncc(C#N)c(Nc3ccc(F)c(...   \n",
       "\n",
       "       Target_ID                                             Target         Y  \n",
       "0         O00141  MTVKTEAAKGTLTYSRMRGMVAILIAFMKQRRMGLNDFIQKIANNS...  11.10000  \n",
       "1         O14920  MSWSPSLTTQTCGAWEMKERLGTGGFGNVIRWHNQETGEQIAIKQC...  11.10000  \n",
       "2         O15111  MERPPGLRPGAGGPWEMRERLGTGGFGNVCLYQHRELDLKIAIKSC...  11.10000  \n",
       "3         P00533  MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFED...  11.10000  \n",
       "4         P04626  MELAALCRWGLLLALLPPGAASTQVCTGTDMKLRLPASPETHLDML...  11.10000  \n",
       "...          ...                                                ...       ...  \n",
       "117652    Q13554  MATTVTCTRFTDEYQLYEDIGKGAFSVVRRCVKLCTGHEYAAKIIN...  10.49794  \n",
       "117653    Q13555  MATTATCTRFTDDYQLFEELGKGAFSVVRRCVKKTSTQEYAAKIIN...  10.49794  \n",
       "117654    Q13557  MASTTTCTRFTDEYQLFEELGKGAFSVVRRCMKIPTGQEYAAKIIN...  10.49794  \n",
       "117655    Q16539  MSQERPTFYRQELNKTIWEVPERYQNLSPVGSGAYGSVCAAFDTKT...  10.49794  \n",
       "117656    Q9UQM7  MATITCTRFTEEYQLFEELGKGAFSVVRRCVKVLAGQEYAAKIINT...  10.49794  \n",
       "\n",
       "[117657 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tdc.multi_pred import DTI\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "major = sys.version_info.major\n",
    "minor = sys.version_info.minor\n",
    "micro = sys.version_info.micro\n",
    "\n",
    "print(f\"Python version: {major}.{minor}.{micro}\")\n",
    "\n",
    "data = DTI(name='KIBA')\n",
    "KibaDataSet = data.get_data()\n",
    "print('number of drugs =',len(KibaDataSet['Drug_ID'].unique()))\n",
    "print('number of proteins = ',len(KibaDataSet[\"Target_ID\"].unique()))\n",
    "KibaDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473cec14-0ebd-4ae7-a9aa-bcaec9e9b685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset had 229 unique proteins.\n",
      "Filtered dataset has 203 unique proteins.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the folder where your protein graphs are saved\n",
    "graph_folder = \"./ProteinGraphs\"\n",
    "\n",
    "# Get the list of protein IDs for which you have graphs (strip the '_graph.pt' suffix)\n",
    "protein_ids_with_graphs = [f.split('_graph.pt')[0] for f in os.listdir(graph_folder) if f.endswith('_graph.pt')]\n",
    "\n",
    "# Convert this list to a set for faster lookups\n",
    "protein_ids_with_graphs = set(protein_ids_with_graphs)\n",
    "\n",
    "\n",
    "# Assuming KibaDataSet is already loaded as a pandas DataFrame\n",
    "valid_protein_dataset = KibaDataSet[KibaDataSet[\"Target_ID\"].isin(protein_ids_with_graphs)]\n",
    "\n",
    "# Save the new dataset with valid proteins\n",
    "valid_protein_dataset.to_csv(\"filtered_KibaDataSet.csv\", index=False)\n",
    "\n",
    "# Check the size of the new dataset\n",
    "print(f\"Original dataset had {len(KibaDataSet['Target_ID'].unique())} unique proteins.\")\n",
    "print(f\"Filtered dataset has {len(valid_protein_dataset['Target_ID'].unique())} unique proteins.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb9e0a9-0a3b-47b7-ade0-5a35b0409aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_protein_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7c81816-af56-4cdd-969f-566c9aa79161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 0 as prepared_samples/sample_0.pt\n",
      "Saved sample 10000 as prepared_samples/sample_10000.pt\n",
      "Saved sample 20000 as prepared_samples/sample_20000.pt\n",
      "Saved sample 30000 as prepared_samples/sample_30000.pt\n",
      "Saved sample 40000 as prepared_samples/sample_40000.pt\n",
      "Saved sample 50000 as prepared_samples/sample_50000.pt\n",
      "Saved sample 60000 as prepared_samples/sample_60000.pt\n",
      "Saved sample 70000 as prepared_samples/sample_70000.pt\n",
      "Saved sample 80000 as prepared_samples/sample_80000.pt\n",
      "Saved sample 90000 as prepared_samples/sample_90000.pt\n",
      "Saved sample 100000 as prepared_samples/sample_100000.pt\n",
      "Dataset preparation completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_graph(path, is_pickle=True):\n",
    "    \"\"\"\n",
    "    Load a molecule graph (.pkl) or a protein graph (.pt).\n",
    "    If is_pickle is True, use pickle to load the file; otherwise, use torch.load.\n",
    "    \"\"\"\n",
    "    if is_pickle:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return torch.load(path)\n",
    "\n",
    "def prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Incrementally prepares the dataset and saves each (molecule, protein, target) tuple as a separate .pt file.\n",
    "    \n",
    "    Args:\n",
    "    - filtered_dataset: The filtered KIBA dataset (DataFrame).\n",
    "    - molecule_graph_dir: Directory where molecule graphs are stored.\n",
    "    - protein_graph_dir: Directory where protein graphs are stored.\n",
    "    - output_dir: Directory to save the prepared dataset incrementally.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for index, row in filtered_dataset.iterrows():\n",
    "        protein_id = row['Target_ID']\n",
    "        chembl_id = row['Drug_ID']\n",
    "        \n",
    "        # Load the protein graph (.pt)\n",
    "        pro_graph_path = os.path.join(protein_graph_dir, f\"{protein_id}_graph.pt\")\n",
    "        if not os.path.exists(pro_graph_path):\n",
    "            print(f\"Protein graph not found: {protein_id}\")\n",
    "            continue\n",
    "        pro_graph = load_graph(pro_graph_path, is_pickle=False)\n",
    "        \n",
    "        # Load the molecule graph (.pkl)\n",
    "        mol_graph_path = os.path.join(molecule_graph_dir, f\"{chembl_id}_graph.pkl\")\n",
    "        if not os.path.exists(mol_graph_path):\n",
    "            print(f\"Molecule graph not found: {chembl_id}\")\n",
    "            continue\n",
    "        mol_graph = load_graph(mol_graph_path)\n",
    "\n",
    "        # Load target (affinity value)\n",
    "        target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "        \n",
    "        # Create the sample as a tuple (molecule graph, protein graph, target)\n",
    "        sample = (mol_graph, pro_graph, target)\n",
    "        \n",
    "        # Save the sample as a .pt file\n",
    "        sample_path = os.path.join(output_dir, f\"sample_{index}.pt\")\n",
    "        torch.save(sample, sample_path)\n",
    "\n",
    "        if(index%10000 == 0 ):\n",
    "            print(f\"Saved sample {index} as {sample_path}\")\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "# Example usage for individual saving\n",
    "molecule_graph_dir = 'molecule_graphs/'  # Directory where molecule graphs are stored\n",
    "protein_graph_dir = 'ProteinGraphs/'  # Directory where protein graphs are stored\n",
    "filtered_dataset_path = 'filtered_KibaDataSet.csv'  # Path to the filtered dataset CSV\n",
    "output_dir = 'prepared_samples/'  # Directory to save individual samples\n",
    "\n",
    "# Load filtered dataset CSV\n",
    "filtered_dataset = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# Prepare the dataset incrementally, saving each sample as a .pt file\n",
    "prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir)\n",
    "\n",
    "print(\"Dataset preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7df79a9-3ea6-4c18-95cd-46858d789e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "\n",
    "# GCN based model\n",
    "class GNNNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro=54, num_features_mol=78, output_dim=128, dropout=0.2):\n",
    "        super(GNNNet, self).__init__()\n",
    "\n",
    "        print('GNNNet Loaded')\n",
    "        self.n_output = n_output\n",
    "        self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "        self.mol_conv2 = GCNConv(num_features_mol, num_features_mol * 2)\n",
    "        self.mol_conv3 = GCNConv(num_features_mol * 2, num_features_mol * 4)\n",
    "        self.mol_fc_g1 = torch.nn.Linear(num_features_mol * 4, 1024)\n",
    "        self.mol_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        # self.pro_conv1 = GCNConv(embed_dim, embed_dim)\n",
    "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro_conv2 = GCNConv(num_features_pro, num_features_pro * 2)\n",
    "        self.pro_conv3 = GCNConv(num_features_pro * 2, num_features_pro * 4)\n",
    "        # self.pro_conv4 = GCNConv(embed_dim * 4, embed_dim * 8)\n",
    "        self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 1024)\n",
    "        self.pro_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        # get graph input\n",
    "        mol_x, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "        # get protein input\n",
    "        target_x, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "\n",
    "        # target_seq=data_pro.target\n",
    "\n",
    "        # print('size')\n",
    "        # print('mol_x', mol_x.size(), 'edge_index', mol_edge_index.size(), 'batch', mol_batch.size())\n",
    "        # print('target_x', target_x.size(), 'target_edge_index', target_batch.size(), 'batch', target_batch.size())\n",
    "\n",
    "        x = self.mol_conv1(mol_x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv2(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv3(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = gep(x, mol_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.mol_fc_g1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_fc_g2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        xt = self.pro_conv1(target_x, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv2(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv3(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # xt = self.pro_conv4(xt, target_edge_index)\n",
    "        # xt = self.relu(xt)\n",
    "        xt = gep(xt, target_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        xt = self.relu(self.pro_fc_g1(xt))\n",
    "        xt = self.dropout(xt)\n",
    "        xt = self.pro_fc_g2(xt)\n",
    "        xt = self.dropout(xt)\n",
    "\n",
    "        # print(\"sizes of out coming gnns\",x.size(), xt.size())\n",
    "        # concat\n",
    "        xc = torch.cat((x, xt), 1)\n",
    "        # print('after concat',xc.size())\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f4f7b4-cf75-4ec3-aeac-a14c988a1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1783ef6-888c-425f-849f-aa5c2304a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda.\n",
      "Created directory for checkpoints at prepared_samples/TrainingModel\n",
      "GNNNet Loaded\n",
      "Model is on device: cuda:0\n",
      "No checkpoint found, starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                          | 1/300 [02:00<10:00:40, 120.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 3.2072\n",
      "Checkpoint saved at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏                         | 2/300 [04:34<11:34:57, 139.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300, Loss: 1.0997\n",
      "Checkpoint saved at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                         | 3/300 [07:24<12:41:37, 153.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300, Loss: 1.0402\n",
      "Checkpoint saved at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                         | 4/300 [09:07<11:00:12, 133.83s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300, Loss: 1.0058\n",
      "Checkpoint saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▍                         | 5/300 [10:53<10:07:57, 123.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300, Loss: 0.9419\n",
      "Checkpoint saved at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▌                          | 6/300 [12:36<9:31:11, 116.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300, Loss: 0.8823\n",
      "Checkpoint saved at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▋                          | 7/300 [14:16<9:03:27, 111.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300, Loss: 0.7982\n",
      "Checkpoint saved at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▋                          | 8/300 [15:58<8:47:49, 108.46s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300, Loss: 0.7433\n",
      "Checkpoint saved at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▊                          | 9/300 [17:38<8:33:07, 105.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300, Loss: 0.7066\n",
      "Checkpoint saved at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▊                         | 10/300 [19:18<8:21:48, 103.82s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300, Loss: 0.6996\n",
      "Checkpoint saved at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▉                         | 11/300 [20:58<8:15:01, 102.77s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300, Loss: 0.6824\n",
      "Checkpoint saved at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|█                         | 12/300 [22:39<8:11:03, 102.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300, Loss: 0.6699\n",
      "Checkpoint saved at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|█▏                        | 13/300 [24:19<8:05:36, 101.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300, Loss: 0.6412\n",
      "Checkpoint saved at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▏                        | 14/300 [26:00<8:02:35, 101.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300, Loss: 0.6424\n",
      "Checkpoint saved at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▎                        | 15/300 [27:39<7:57:49, 100.60s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300, Loss: 0.6389\n",
      "Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▍                         | 16/300 [29:16<7:52:02, 99.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/300, Loss: 0.6116\n",
      "Checkpoint saved at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|█▌                         | 17/300 [30:54<7:46:45, 98.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/300, Loss: 0.6039\n",
      "Checkpoint saved at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|█▌                         | 18/300 [32:30<7:41:59, 98.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/300, Loss: 0.5982\n",
      "Checkpoint saved at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|█▋                         | 19/300 [34:07<7:38:28, 97.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/300, Loss: 0.5925\n",
      "Checkpoint saved at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|█▊                         | 20/300 [35:45<7:36:43, 97.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/300, Loss: 0.5704\n",
      "Checkpoint saved at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|█▉                         | 21/300 [37:23<7:35:06, 97.87s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/300, Loss: 0.5551\n",
      "Checkpoint saved at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|█▉                         | 22/300 [39:01<7:33:02, 97.78s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/300, Loss: 0.5468\n",
      "Checkpoint saved at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|██                         | 23/300 [40:40<7:33:06, 98.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/300, Loss: 0.5375\n",
      "Checkpoint saved at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|██▏                        | 24/300 [42:22<7:37:42, 99.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300, Loss: 0.5297\n",
      "Checkpoint saved at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|██▎                        | 25/300 [44:03<7:37:41, 99.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/300, Loss: 0.5262\n",
      "Checkpoint saved at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██▎                       | 26/300 [45:47<7:41:47, 101.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300, Loss: 0.5182\n",
      "Checkpoint saved at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██▎                       | 27/300 [47:27<7:38:32, 100.78s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300, Loss: 0.5117\n",
      "Checkpoint saved at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|██▍                       | 28/300 [49:06<7:34:36, 100.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/300, Loss: 0.5043\n",
      "Checkpoint saved at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██▌                        | 29/300 [50:44<7:30:18, 99.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/300, Loss: 0.4969\n",
      "Checkpoint saved at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██▋                        | 30/300 [52:22<7:25:09, 98.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/300, Loss: 0.4932\n",
      "Checkpoint saved at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|██▊                        | 31/300 [53:58<7:19:45, 98.09s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/300, Loss: 0.4934\n",
      "Checkpoint saved at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|██▉                        | 32/300 [55:35<7:17:38, 97.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300, Loss: 0.4865\n",
      "Checkpoint saved at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|██▉                        | 33/300 [57:13<7:14:56, 97.74s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300, Loss: 0.4762\n",
      "Checkpoint saved at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|███                        | 34/300 [58:49<7:11:51, 97.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300, Loss: 0.4742\n",
      "Checkpoint saved at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|██▉                      | 35/300 [1:00:27<7:10:30, 97.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300, Loss: 0.4693\n",
      "Checkpoint saved at epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███                      | 36/300 [1:02:03<7:07:16, 97.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300, Loss: 0.4623\n",
      "Checkpoint saved at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|███                      | 37/300 [1:03:38<7:03:12, 96.55s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300, Loss: 0.4530\n",
      "Checkpoint saved at epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|███▏                     | 38/300 [1:05:12<6:58:13, 95.78s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300, Loss: 0.4573\n",
      "Checkpoint saved at epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|███▎                     | 39/300 [1:06:48<6:56:18, 95.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300, Loss: 0.4429\n",
      "Checkpoint saved at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|███▎                     | 40/300 [1:08:25<6:56:37, 96.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300, Loss: 0.4473\n",
      "Checkpoint saved at epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|███▍                     | 41/300 [1:10:02<6:56:40, 96.53s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300, Loss: 0.4413\n",
      "Checkpoint saved at epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|███▌                     | 42/300 [1:11:39<6:54:53, 96.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300, Loss: 0.4304\n",
      "Checkpoint saved at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|███▌                     | 43/300 [1:13:14<6:52:08, 96.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300, Loss: 0.4298\n",
      "Checkpoint saved at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|███▋                     | 44/300 [1:14:50<6:49:11, 95.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300, Loss: 0.4377\n",
      "Checkpoint saved at epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|███▊                     | 45/300 [1:16:25<6:46:43, 95.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300, Loss: 0.4230\n",
      "Checkpoint saved at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|███▊                     | 46/300 [1:18:01<6:45:19, 95.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300, Loss: 0.4132\n",
      "Checkpoint saved at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|███▉                     | 47/300 [1:19:38<6:46:01, 96.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300, Loss: 0.4125\n",
      "Checkpoint saved at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|████                     | 48/300 [1:21:16<6:46:32, 96.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300, Loss: 0.4070\n",
      "Checkpoint saved at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|████                     | 49/300 [1:22:53<6:44:38, 96.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300, Loss: 0.3981\n",
      "Checkpoint saved at epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|████▏                    | 50/300 [1:24:28<6:40:52, 96.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300, Loss: 0.3907\n",
      "Checkpoint saved at epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|████▎                    | 51/300 [1:26:05<6:40:50, 96.59s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300, Loss: 0.3897\n",
      "Checkpoint saved at epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|████▎                    | 52/300 [1:27:41<6:38:32, 96.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300, Loss: 0.3851\n",
      "Checkpoint saved at epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|████▍                    | 53/300 [1:29:19<6:38:20, 96.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300, Loss: 0.3830\n",
      "Checkpoint saved at epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|████▌                    | 54/300 [1:30:55<6:36:21, 96.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300, Loss: 0.3772\n",
      "Checkpoint saved at epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|████▌                    | 55/300 [1:32:33<6:36:11, 97.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300, Loss: 0.3763\n",
      "Checkpoint saved at epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|████▋                    | 56/300 [1:34:14<6:39:20, 98.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300, Loss: 0.3739\n",
      "Checkpoint saved at epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|████▊                    | 57/300 [1:35:57<6:42:55, 99.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300, Loss: 0.3673\n",
      "Checkpoint saved at epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|████▋                   | 58/300 [1:37:39<6:44:44, 100.35s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300, Loss: 0.3602\n",
      "Checkpoint saved at epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|████▋                   | 59/300 [1:39:20<6:43:45, 100.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300, Loss: 0.3578\n",
      "Checkpoint saved at epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|████▊                   | 60/300 [1:41:01<6:42:13, 100.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300, Loss: 0.3543\n",
      "Checkpoint saved at epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|████▉                   | 61/300 [1:42:44<6:43:57, 101.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300, Loss: 0.3481\n",
      "Checkpoint saved at epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|█████▏                   | 62/300 [1:44:20<6:36:21, 99.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300, Loss: 0.3476\n",
      "Checkpoint saved at epoch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|█████▎                   | 63/300 [1:45:58<6:31:53, 99.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300, Loss: 0.3431\n",
      "Checkpoint saved at epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|█████▎                   | 64/300 [1:47:33<6:25:34, 98.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300, Loss: 0.3384\n",
      "Checkpoint saved at epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|█████▍                   | 65/300 [1:49:09<6:20:51, 97.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300, Loss: 0.3373\n",
      "Checkpoint saved at epoch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|█████▌                   | 66/300 [1:50:43<6:16:04, 96.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300, Loss: 0.3297\n",
      "Checkpoint saved at epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|█████▌                   | 67/300 [1:52:17<6:11:20, 95.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300, Loss: 0.3254\n",
      "Checkpoint saved at epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|█████▋                   | 68/300 [1:53:51<6:08:14, 95.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300, Loss: 0.3229\n",
      "Checkpoint saved at epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|█████▊                   | 69/300 [1:55:28<6:07:59, 95.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300, Loss: 0.3218\n",
      "Checkpoint saved at epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|█████▊                   | 70/300 [1:57:04<6:07:27, 95.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300, Loss: 0.3223\n",
      "Checkpoint saved at epoch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|█████▉                   | 71/300 [1:58:41<6:06:34, 96.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300, Loss: 0.3124\n",
      "Checkpoint saved at epoch 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██████                   | 72/300 [2:00:17<6:05:12, 96.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300, Loss: 0.3127\n",
      "Checkpoint saved at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██████                   | 73/300 [2:01:52<6:02:38, 95.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300, Loss: 0.3082\n",
      "Checkpoint saved at epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██████▏                  | 74/300 [2:03:26<5:58:34, 95.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300, Loss: 0.3069\n",
      "Checkpoint saved at epoch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██████▎                  | 75/300 [2:05:01<5:56:28, 95.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300, Loss: 0.3031\n",
      "Checkpoint saved at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██████▎                  | 76/300 [2:06:36<5:55:14, 95.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300, Loss: 0.3005\n",
      "Checkpoint saved at epoch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██████▍                  | 77/300 [2:08:12<5:54:48, 95.46s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300, Loss: 0.2989\n",
      "Checkpoint saved at epoch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██████▌                  | 78/300 [2:09:49<5:54:34, 95.83s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300, Loss: 0.2951\n",
      "Checkpoint saved at epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██████▌                  | 79/300 [2:11:25<5:53:02, 95.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300, Loss: 0.2924\n",
      "Checkpoint saved at epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██████▋                  | 80/300 [2:12:59<5:50:02, 95.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300, Loss: 0.2904\n",
      "Checkpoint saved at epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██████▊                  | 81/300 [2:14:34<5:47:22, 95.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300, Loss: 0.2861\n",
      "Checkpoint saved at epoch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██████▊                  | 82/300 [2:16:07<5:43:32, 94.55s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300, Loss: 0.2865\n",
      "Checkpoint saved at epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██████▉                  | 83/300 [2:17:40<5:40:41, 94.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300, Loss: 0.2841\n",
      "Checkpoint saved at epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|███████                  | 84/300 [2:19:14<5:38:33, 94.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300, Loss: 0.2793\n",
      "Checkpoint saved at epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|███████                  | 85/300 [2:20:50<5:39:29, 94.74s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300, Loss: 0.2761\n",
      "Checkpoint saved at epoch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|███████▏                 | 86/300 [2:22:28<5:40:52, 95.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300, Loss: 0.2735\n",
      "Checkpoint saved at epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|███████▏                 | 87/300 [2:24:04<5:39:37, 95.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300, Loss: 0.2732\n",
      "Checkpoint saved at epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|███████▎                 | 88/300 [2:25:40<5:38:28, 95.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300, Loss: 0.2747\n",
      "Checkpoint saved at epoch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███████▍                 | 89/300 [2:27:15<5:35:58, 95.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300, Loss: 0.2705\n",
      "Checkpoint saved at epoch 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███████▌                 | 90/300 [2:28:48<5:32:33, 95.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300, Loss: 0.2656\n",
      "Checkpoint saved at epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███████▌                 | 91/300 [2:30:22<5:29:08, 94.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/300, Loss: 0.2650\n",
      "Checkpoint saved at epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███████▋                 | 92/300 [2:31:55<5:25:59, 94.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/300, Loss: 0.2645\n",
      "Checkpoint saved at epoch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███████▊                 | 93/300 [2:33:28<5:23:56, 93.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300, Loss: 0.2618\n",
      "Checkpoint saved at epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███████▊                 | 94/300 [2:35:02<5:22:32, 93.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300, Loss: 0.2597\n",
      "Checkpoint saved at epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███████▉                 | 95/300 [2:36:38<5:22:45, 94.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/300, Loss: 0.2586\n",
      "Checkpoint saved at epoch 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|████████                 | 96/300 [2:38:15<5:24:11, 95.35s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/300, Loss: 0.2543\n",
      "Checkpoint saved at epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|████████                 | 97/300 [2:39:50<5:22:08, 95.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300, Loss: 0.2551\n",
      "Checkpoint saved at epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|████████▏                | 98/300 [2:41:24<5:19:10, 94.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300, Loss: 0.2509\n",
      "Checkpoint saved at epoch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|████████▎                | 99/300 [2:42:58<5:16:17, 94.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300, Loss: 0.2484\n",
      "Checkpoint saved at epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|████████                | 100/300 [2:44:31<5:13:59, 94.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300, Loss: 0.2481\n",
      "Checkpoint saved at epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|████████                | 101/300 [2:46:06<5:13:18, 94.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300, Loss: 0.2478\n",
      "Checkpoint saved at epoch 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|████████▏               | 102/300 [2:47:42<5:12:28, 94.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/300, Loss: 0.2441\n",
      "Checkpoint saved at epoch 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|████████▏               | 103/300 [2:49:17<5:11:48, 94.97s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300, Loss: 0.2455\n",
      "Checkpoint saved at epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|████████▎               | 104/300 [2:50:53<5:10:54, 95.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300, Loss: 0.2418\n",
      "Checkpoint saved at epoch 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|████████▍               | 105/300 [2:52:28<5:08:42, 94.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/300, Loss: 0.2424\n",
      "Checkpoint saved at epoch 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|████████▍               | 106/300 [2:54:01<5:05:58, 94.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/300, Loss: 0.2393\n",
      "Checkpoint saved at epoch 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|████████▌               | 107/300 [2:55:34<5:02:40, 94.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300, Loss: 0.2357\n",
      "Checkpoint saved at epoch 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|████████▋               | 108/300 [2:57:08<5:00:56, 94.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300, Loss: 0.2345\n",
      "Checkpoint saved at epoch 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|████████▋               | 109/300 [2:58:41<4:58:21, 93.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300, Loss: 0.2337\n",
      "Checkpoint saved at epoch 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|████████▊               | 110/300 [3:00:14<4:56:05, 93.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300, Loss: 0.2312\n",
      "Checkpoint saved at epoch 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|████████▉               | 111/300 [3:01:48<4:55:07, 93.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300, Loss: 0.2316\n",
      "Checkpoint saved at epoch 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|████████▉               | 112/300 [3:03:23<4:54:46, 94.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300, Loss: 0.2306\n",
      "Checkpoint saved at epoch 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|█████████               | 113/300 [3:04:58<4:53:51, 94.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/300, Loss: 0.2284\n",
      "Checkpoint saved at epoch 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|█████████               | 114/300 [3:06:32<4:52:20, 94.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300, Loss: 0.2261\n",
      "Checkpoint saved at epoch 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|█████████▏              | 115/300 [3:08:06<4:50:23, 94.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300, Loss: 0.2260\n",
      "Checkpoint saved at epoch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|█████████▎              | 116/300 [3:09:40<4:48:26, 94.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300, Loss: 0.2228\n",
      "Checkpoint saved at epoch 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|█████████▎              | 117/300 [3:11:13<4:45:52, 93.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300, Loss: 0.2225\n",
      "Checkpoint saved at epoch 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|█████████▍              | 118/300 [3:12:48<4:45:20, 94.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300, Loss: 0.2208\n",
      "Checkpoint saved at epoch 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|█████████▌              | 119/300 [3:14:27<4:48:17, 95.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/300, Loss: 0.2193\n",
      "Checkpoint saved at epoch 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|█████████▌              | 119/300 [3:15:58<4:58:04, 98.81s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 88\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(sample_dir, num_epochs, test_size, lr)\u001b[0m\n\u001b[1;32m     85\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target_list, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpro_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target)\n\u001b[1;32m     90\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[11], line 80\u001b[0m, in \u001b[0;36mGNNNet.forward\u001b[0;34m(self, data_mol, data_pro)\u001b[0m\n\u001b[1;32m     76\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(xt)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# xt = self.pro_conv4(xt, target_edge_index)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# xt = self.relu(xt)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[43mgep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# global pooling\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# flatten\u001b[39;00m\n\u001b[1;32m     83\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpro_fc_g1(xt))\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch_geometric/nn/pool/glob.py:63\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py:53\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `dim` argument must lay between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# For now, we maintain various different code paths, based on whether\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# the input requires gradients and whether it lays on the CPU/GPU.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# For example, `torch_scatter` is usually faster than\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# indices, but is therefore way slower in its backward implementation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# More insights can be found in `test/utils/test_scatter.py`.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize()[:dim] \u001b[38;5;241m+\u001b[39m (dim_size, ) \u001b[38;5;241m+\u001b[39m src\u001b[38;5;241m.\u001b[39msize()[dim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate metrics on training and testing data at the end of training\n",
    "def train_and_evaluate(sample_dir, num_epochs=100, test_size=0.2, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "\n",
    "    # Split data into training and test sets\n",
    "    train_files, test_files = train_test_split(sample_files, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create a directory for the model checkpoint\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    # Determine input feature dimensions from your data\n",
    "    sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "\n",
    "    num_features_mol = mol_data.x.size(1)\n",
    "    num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "    # Initialize the GNN model with correct input dimensions\n",
    "    model = GNNNet(\n",
    "        num_features_mol=num_features_mol,\n",
    "        num_features_pro=num_features_pro\n",
    "    ).to(device)\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Initialize starting epoch\n",
    "    start_epoch = 1\n",
    "\n",
    "    # Check for existing checkpoints in TrainingModel directory\n",
    "    existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                            if f.endswith('.pt') and f.startswith('model_epoch')]\n",
    "\n",
    "    if existing_checkpoints:\n",
    "        # Find the latest checkpoint based on epoch number\n",
    "        latest_checkpoint = max(existing_checkpoints,\n",
    "                                key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "        checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        loaded_epoch = checkpoint['epoch']\n",
    "        start_epoch = loaded_epoch + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "    # Training loop with progress bar over epochs\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                      desc=\"Training\", unit=\"epoch\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Prepare batch loader\n",
    "        batch_size = 200  # Adjust batch size as needed\n",
    "        batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "        for batch_samples in batch_loader_iter:\n",
    "            mol_data_list = []\n",
    "            pro_data_list = []\n",
    "            target_list = []\n",
    "\n",
    "            for sample in batch_samples:\n",
    "                mol_data = sample[0]\n",
    "                pro_data = sample[1]\n",
    "                target = sample[2]\n",
    "\n",
    "                mol_data_list.append(mol_data)\n",
    "                pro_data_list.append(pro_data)\n",
    "                target_list.append(target)\n",
    "\n",
    "            mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "            pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "            target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(mol_batch, pro_batch)\n",
    "            loss = loss_fn(output.view(-1), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "        avg_loss = running_loss / len(train_files)\n",
    "        tqdm.write(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save the model and optimizer states after each epoch\n",
    "        checkpoint_filename = f\"model_epoch{epoch}.pt\"\n",
    "        checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        tqdm.write(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "    # Final evaluation on test data\n",
    "    print(\"\\nEvaluating model after training...\")\n",
    "    model.eval()\n",
    "\n",
    "    def evaluate(files):\n",
    "        total_preds, total_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            batch_loader_iter = batch_loader(files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                total_preds.append(output.cpu().numpy())\n",
    "                total_labels.append(target.cpu().numpy())\n",
    "\n",
    "        # Convert lists to numpy arrays for evaluation\n",
    "        total_preds = np.concatenate(total_preds)\n",
    "        total_labels = np.concatenate(total_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        mse = get_mse(total_labels, total_preds)\n",
    "        ci = get_ci(total_labels, total_preds)\n",
    "        pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "        return mse, ci, pearson\n",
    "\n",
    "    train_mse, train_ci, train_pearson = evaluate(train_files)\n",
    "    test_mse, test_ci, test_pearson = evaluate(test_files)\n",
    "\n",
    "    print(f\"Final Training Metrics: MSE: {train_mse:.4f}, CI: {train_ci:.4f}, Pearson: {train_pearson:.4f}\")\n",
    "    print(f\"Final Test Metrics: MSE: {test_mse:.4f}, CI: {test_ci:.4f}, Pearson: {test_pearson:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 300  # Adjust the number of epochs as needed\n",
    "    test_size = 0.2   # Proportion of the dataset to include in the test split\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    train_and_evaluate(\n",
    "        sample_dir,\n",
    "        num_epochs=num_epochs,\n",
    "        test_size=test_size,\n",
    "        lr=learning_rate\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e639f-272a-44db-ab25-283f93c68d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53639e7d-fd17-4cb7-823c-a774081f4ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1beaf7-8e97-4f81-9798-c76e379a780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "#     num_epochs = 300  # Adjust the number of epochs as needed\n",
    "#     test_size = 0.2   # Proportion of the dataset to include in the test split\n",
    "#     learning_rate = 0.001  # Learning rate\n",
    "\n",
    "#     # Run the training function\n",
    "#     train_mse_list, train_ci_list, train_pearson_list, test_mse_list, test_ci_list, test_pearson_list = train_and_evaluate(\n",
    "#         sample_dir,\n",
    "#         num_epochs=num_epochs,\n",
    "#         test_size=test_size,\n",
    "#         lr=learning_rate\n",
    "#     )\n",
    "\n",
    "#     # Optionally, plot the metrics over epochs\n",
    "#     # For example, using matplotlib\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     epochs = range(1, num_epochs + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "\n",
    "#     # Plot MSE\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.plot(epochs, train_mse_list, label='Train MSE')\n",
    "#     plt.plot(epochs, test_mse_list, label='Test MSE')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('MSE')\n",
    "#     plt.title('MSE over Epochs')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plot CI\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.plot(epochs, train_ci_list, label='Train CI')\n",
    "#     plt.plot(epochs, test_ci_list, label='Test CI')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Concordance Index (CI)')\n",
    "#     plt.title('CI over Epochs')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plot Pearson Correlation\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.plot(epochs, train_pearson_list, label='Train Pearson')\n",
    "#     plt.plot(epochs, test_pearson_list, label='Test Pearson')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Pearson Correlation')\n",
    "#     plt.title('Pearson Correlation over Epochs')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf2234-2142-43c4-839d-a994489d5b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
