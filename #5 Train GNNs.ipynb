{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "467fa705-129e-4d80-8df5-108b10e3e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "\n",
    "# # GNN Model definition for molecule and protein graphs\n",
    "# class GNNNet(torch.nn.Module):\n",
    "#     def __init__(self, n_output=1, num_features_pro=52, num_features_mol=78, output_dim=128, dropout=0.2):\n",
    "#         super(GNNNet, self).__init__()\n",
    "\n",
    "#         print('GNNNet Loaded')\n",
    "#         # Molecule GNN layers\n",
    "#         self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "#         self.mol_conv2 = GCNConv(num_features_mol, num_features_mol * 2)\n",
    "#         self.mol_conv3 = GCNConv(num_features_mol * 2, num_features_mol * 4)\n",
    "#         self.mol_fc_g1 = torch.nn.Linear(num_features_mol * 4, 1024)\n",
    "#         self.mol_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "#         # Protein GNN layers\n",
    "#         self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "#         self.pro_conv2 = GCNConv(num_features_pro * 2, num_features_pro * 4)\n",
    "#         self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 1024)\n",
    "#         self.pro_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#         # Combined dense layers\n",
    "#         self.fc1 = nn.Linear(2 * output_dim, 1024)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.out = nn.Linear(512, n_output)\n",
    "\n",
    "#     def forward(self, data_mol, data_pro):\n",
    "#         # Molecule forward pass\n",
    "#         mol_x, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "#         mol_x = self.mol_conv1(mol_x, mol_edge_index)\n",
    "#         mol_x = self.relu(mol_x)\n",
    "#         mol_x = self.mol_conv2(mol_x, mol_edge_index)\n",
    "#         mol_x = self.relu(mol_x)\n",
    "#         mol_x = self.mol_conv3(mol_x, mol_edge_index)\n",
    "#         mol_x = gep(mol_x, mol_batch)  # global pooling\n",
    "#         mol_x = self.relu(self.mol_fc_g1(mol_x))\n",
    "#         mol_x = self.dropout(mol_x)\n",
    "#         mol_x = self.mol_fc_g2(mol_x)\n",
    "\n",
    "#         # Protein forward pass\n",
    "#         pro_x, pro_edge_index, pro_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "#         pro_x = self.pro_conv1(pro_x, pro_edge_index)\n",
    "#         pro_x = self.relu(pro_x)\n",
    "#         pro_x = self.pro_conv2(pro_x, pro_edge_index)\n",
    "#         pro_x = self.relu(pro_x)\n",
    "#         pro_x = gep(pro_x, pro_batch)  # global pooling\n",
    "#         pro_x = self.relu(self.pro_fc_g1(pro_x))\n",
    "#         pro_x = self.dropout(pro_x)\n",
    "#         pro_x = self.pro_fc_g2(pro_x)\n",
    "\n",
    "#         # Concatenate molecule and protein features\n",
    "#         xc = torch.cat((mol_x, pro_x), dim=1)\n",
    "#         xc = self.fc1(xc)\n",
    "#         xc = self.relu(xc)\n",
    "#         xc = self.dropout(xc)\n",
    "#         xc = self.fc2(xc)\n",
    "#         xc = self.relu(xc)\n",
    "#         xc = self.dropout(xc)\n",
    "#         out = self.out(xc)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b22be849-efeb-44d1-a6b2-a88aea652a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "\n",
    "# GCN based model\n",
    "class GNNNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro=54, num_features_mol=78, output_dim=128, dropout=0.2):\n",
    "        super(GNNNet, self).__init__()\n",
    "\n",
    "        print('GNNNet Loaded')\n",
    "        self.n_output = n_output\n",
    "        self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "        self.mol_conv2 = GCNConv(num_features_mol, num_features_mol * 2)\n",
    "        self.mol_conv3 = GCNConv(num_features_mol * 2, num_features_mol * 4)\n",
    "        self.mol_fc_g1 = torch.nn.Linear(num_features_mol * 4, 1024)\n",
    "        self.mol_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        # self.pro_conv1 = GCNConv(embed_dim, embed_dim)\n",
    "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro_conv2 = GCNConv(num_features_pro, num_features_pro * 2)\n",
    "        self.pro_conv3 = GCNConv(num_features_pro * 2, num_features_pro * 4)\n",
    "        # self.pro_conv4 = GCNConv(embed_dim * 4, embed_dim * 8)\n",
    "        self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 1024)\n",
    "        self.pro_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        # get graph input\n",
    "        mol_x, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "        # get protein input\n",
    "        target_x, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "\n",
    "        # target_seq=data_pro.target\n",
    "\n",
    "        # print('size')\n",
    "        # print('mol_x', mol_x.size(), 'edge_index', mol_edge_index.size(), 'batch', mol_batch.size())\n",
    "        # print('target_x', target_x.size(), 'target_edge_index', target_batch.size(), 'batch', target_batch.size())\n",
    "\n",
    "        x = self.mol_conv1(mol_x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv2(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv3(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = gep(x, mol_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.mol_fc_g1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_fc_g2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        xt = self.pro_conv1(target_x, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv2(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv3(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # xt = self.pro_conv4(xt, target_edge_index)\n",
    "        # xt = self.relu(xt)\n",
    "        xt = gep(xt, target_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        xt = self.relu(self.pro_fc_g1(xt))\n",
    "        xt = self.dropout(xt)\n",
    "        xt = self.pro_fc_g2(xt)\n",
    "        xt = self.dropout(xt)\n",
    "\n",
    "        # print(x.size(), xt.size())\n",
    "        # concat\n",
    "        xc = torch.cat((x, xt), 1)\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350d65d3-c80c-434f-a545-80562a3855c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_graph(path, is_pickle=True):\n",
    "#     \"\"\"\n",
    "#     Load a molecule graph (.pkl) or a protein graph (.pt).\n",
    "#     If is_pickle is True, use pickle to load the file; otherwise, use torch.load.\n",
    "#     \"\"\"\n",
    "#     if is_pickle:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             return pickle.load(f)\n",
    "#     else:\n",
    "#         return torch.load(path)\n",
    "\n",
    "# def prepare_dataset_incremental(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_file):\n",
    "#     \"\"\"\n",
    "#     Incrementally prepares the dataset to avoid memory issues.\n",
    "#     Processes one protein and its associated molecules at a time, and appends the results to the output file.\n",
    "    \n",
    "#     Args:\n",
    "#     - filtered_dataset: The filtered KIBA dataset (DataFrame).\n",
    "#     - molecule_graph_dir: Directory where molecule graphs are stored.\n",
    "#     - protein_graph_dir: Directory where protein graphs are stored.\n",
    "#     - output_file: File to save the prepared dataset incrementally.\n",
    "#     \"\"\"\n",
    "#     current_protein = None\n",
    "#     dataset = []\n",
    "    \n",
    "#     for index, row in filtered_dataset.iterrows():\n",
    "#         protein_id = row['Target_ID']\n",
    "#         chembl_id = row['Drug_ID']\n",
    "        \n",
    "#         # If the protein changes, save the dataset for the previous protein\n",
    "#         if current_protein is not None and current_protein != protein_id:\n",
    "#             with open(output_file, 'ab') as f:  # Append to the output file\n",
    "#                 pickle.dump(dataset, f)\n",
    "#             print(f\"Processed and saved data for protein {current_protein}.\")\n",
    "#             dataset = []  # Reset dataset for the next protein\n",
    "        \n",
    "#         current_protein = protein_id\n",
    "        \n",
    "#         # Load the protein graph (.pt)\n",
    "#         pro_graph_path = os.path.join(protein_graph_dir, f\"{protein_id}_graph.pt\")\n",
    "#         if not os.path.exists(pro_graph_path):\n",
    "#             print(f\"Protein graph not found: {protein_id}\")\n",
    "#             continue\n",
    "#         pro_graph = load_graph(pro_graph_path, is_pickle=False)\n",
    "        \n",
    "#         # Load the molecule graph (.pkl)\n",
    "#         mol_graph_path = os.path.join(molecule_graph_dir, f\"{chembl_id}_graph.pkl\")\n",
    "#         if not os.path.exists(mol_graph_path):\n",
    "#             print(f\"Molecule graph not found: {chembl_id}\")\n",
    "#             continue\n",
    "#         mol_graph = load_graph(mol_graph_path)\n",
    "\n",
    "#         # Load target (affinity value)\n",
    "#         target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "        \n",
    "#         # Append the (molecule, protein, target) tuple to the dataset\n",
    "#         dataset.append((mol_graph, pro_graph, target))\n",
    "    \n",
    "#     # Save the last batch (for the final protein)\n",
    "#     if len(dataset) > 0:\n",
    "#         with open(output_file, 'ab') as f:\n",
    "#             pickle.dump(dataset, f)\n",
    "#         print(f\"Processed and saved data for protein {current_protein}.\")\n",
    "\n",
    "# # Example usage for incremental dataset preparation\n",
    "# molecule_graph_dir = 'molecule_graphs/'  # Directory where molecule graphs are stored\n",
    "# protein_graph_dir = 'ProteinGraphs/'  # Directory where protein graphs are stored\n",
    "# filtered_dataset_path = 'filtered_KibaDataSet.csv'  # Path to the filtered dataset CSV\n",
    "\n",
    "# # Load filtered dataset CSV\n",
    "# filtered_dataset = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# # Prepare the dataset incrementally, saving after each protein\n",
    "# output_file = 'incremental_prepared_dataset.pkl'  # Output file to save dataset incrementally\n",
    "# prepare_dataset_incremental(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_file)\n",
    "\n",
    "# print(\"Dataset preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f581c8-63ab-4089-847e-490424dc9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def load_prepared_dataset(filepath):\n",
    "#     \"\"\"\n",
    "#     Load the prepared dataset saved as a pickle file.\n",
    "    \n",
    "#     Args:\n",
    "#     - filepath: Path to the file where the prepared dataset is saved.\n",
    "    \n",
    "#     Returns:\n",
    "#     - dataset: The complete list of samples from the dataset.\n",
    "#     \"\"\"\n",
    "#     dataset = []\n",
    "    \n",
    "#     with open(filepath, 'rb') as f:\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 # Load a batch of samples\n",
    "#                 batch = pickle.load(f)\n",
    "#                 dataset.extend(batch)  # Add the batch to the full dataset\n",
    "#             except EOFError:\n",
    "#                 # End of file reached\n",
    "#                 break\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "# # Example usage:\n",
    "# prepared_dataset_path = 'incremental_prepared_dataset.pkl'  # Path to the saved dataset\n",
    "\n",
    "# # Load the dataset\n",
    "# loaded_dataset = load_prepared_dataset(prepared_dataset_path)\n",
    "\n",
    "# # Print the total number of samples\n",
    "# print(f\"Total number of samples in the dataset: {len(loaded_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edb9a76-684e-4759-ac13-522d84667696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# def load_graph(path):\n",
    "#     # Load graphs from .pkl for molecules and .pt for proteins\n",
    "#     with open(path, 'rb') as f:\n",
    "#         return pickle.load(f)\n",
    "\n",
    "# def prepare_dataset(filtered_dataset, molecule_graph_dir, protein_graph_dir):\n",
    "#     dataset = []\n",
    "    \n",
    "#     for index, row in filtered_dataset.iterrows():\n",
    "#         # Load molecule graph based on Drug_ID\n",
    "#         mol_graph_path = os.path.join(molecule_graph_dir, f\"{row['Drug_ID']}_graph.pkl\")\n",
    "#         mol_graph = load_graph(mol_graph_path)\n",
    "\n",
    "#         # Load protein graph based on Target_ID\n",
    "#         pro_graph_path = os.path.join(protein_graph_dir, f\"{row['Target_ID']}_graph.pt\")\n",
    "#         pro_graph = torch.load(pro_graph_path)\n",
    "        \n",
    "#         # Load target (affinity value)\n",
    "#         target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "        \n",
    "#         # Append tuple (mol_graph, pro_graph, target) to dataset\n",
    "#         dataset.append((mol_graph, pro_graph, target))\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# # Example usage for dataset preparation\n",
    "# molecule_graph_dir = 'molecule_graphs/'  # Directory where molecule graphs are stored\n",
    "# protein_graph_dir = 'ProteinGraphs/'  # Directory where protein graphs are stored\n",
    "# filtered_dataset_path = 'filtered_KibaDataSet.csv'  # Path to the filtered dataset CSV\n",
    "\n",
    "# # Load filtered dataset CSV\n",
    "# filtered_dataset = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# # Prepare the dataset with molecule, protein graphs, and affinity scores\n",
    "# prepared_dataset = prepare_dataset(filtered_dataset, molecule_graph_dir, protein_graph_dir)\n",
    "\n",
    "# # Save the prepared dataset for later usage\n",
    "# torch.save(prepared_dataset, 'prepared_dataset.pt')\n",
    "\n",
    "# print(f\"Dataset prepared with {len(prepared_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d687181-ad6d-4b46-8c7c-8710b7c03086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from torch_geometric.data import DataLoader\n",
    "# import torch.optim as optim\n",
    "# from torch.nn import MSELoss\n",
    "\n",
    "# def train_5fold_cross_validation(prepared_dataset, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Running on {device}.\")\n",
    "\n",
    "#     kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "#     results = []\n",
    "#     loss_fn = MSELoss()\n",
    "    \n",
    "#     for fold, (train_idx, test_idx) in enumerate(kfold.split(prepared_dataset)):\n",
    "#         print(f'Fold {fold + 1}/{n_splits}')\n",
    "        \n",
    "#         # Split the dataset into training and testing based on indices\n",
    "#         train_data = [prepared_dataset[i] for i in train_idx]\n",
    "#         test_data = [prepared_dataset[i] for i in test_idx]\n",
    "        \n",
    "#         # Create DataLoader for training and testing\n",
    "#         train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "#         test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "#         # Initialize the GNN model\n",
    "#         model = GNNNet().to(device)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#         for epoch in range(num_epochs):\n",
    "#             model.train()\n",
    "#             for mol_data, pro_data, target in train_loader:\n",
    "#                 mol_data = mol_data.to(device)\n",
    "#                 pro_data = pro_data.to(device)\n",
    "#                 target = target.to(device)\n",
    "                \n",
    "#                 optimizer.zero_grad()\n",
    "#                 output = model(mol_data, pro_data)\n",
    "#                 loss = loss_fn(output, target)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "#         # Evaluation on the test set\n",
    "#         model.eval()\n",
    "#         total_preds, total_labels = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for mol_data, pro_data, target in test_loader:\n",
    "#                 mol_data = mol_data.to(device)\n",
    "#                 pro_data = pro_data.to(device)\n",
    "#                 target = target.to(device)\n",
    "                \n",
    "#                 output = model(mol_data, pro_data)\n",
    "#                 total_preds.append(output.cpu().numpy())\n",
    "#                 total_labels.append(target.cpu().numpy())\n",
    "\n",
    "#         mse = get_mse(total_labels, total_preds)\n",
    "#         ci = get_ci(total_labels, total_preds)\n",
    "#         pearson = get_pearson(total_labels, total_preds)\n",
    "#         print(f\"Fold {fold+1} - MSE: {mse}, CI: {ci}, Pearson: {pearson}\")\n",
    "\n",
    "#         # Store results for this fold\n",
    "#         results.append((mse, ci, pearson))\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Load the prepared dataset\n",
    "# prepared_dataset = torch.load('prepared_dataset.pt')\n",
    "\n",
    "# # Run 5-fold cross-validation training with the pre-prepared dataset\n",
    "# results = train_5fold_cross_validation(prepared_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "180ee8b6-b3a3-438c-9e53-8a88097785e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "\n",
    "# def load_prepared_dataset(directory):\n",
    "#     \"\"\"\n",
    "#     Load all individual samples from the directory where each sample is saved as a .pt file.\n",
    "    \n",
    "#     Parameters:\n",
    "#     directory (str): Path to the directory containing the individual .pt files\n",
    "    \n",
    "#     Returns:\n",
    "#     list: A list of loaded samples\n",
    "#     \"\"\"\n",
    "#     dataset = []\n",
    "    \n",
    "#     # Iterate over all files in the directory\n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.pt'):\n",
    "#             file_path = os.path.join(directory, filename)\n",
    "#             sample = torch.load(file_path)\n",
    "#             dataset.append(sample)\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "# # Load the prepared dataset from individual .pt files\n",
    "# prepared_samples_dir = 'prepared_samples'  # The directory containing individual sample .pt files\n",
    "# prepared_dataset = load_prepared_dataset(prepared_samples_dir)\n",
    "\n",
    "# # Check how many samples are loaded\n",
    "# print(f\"Loaded {len(prepared_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6c547a0-b012-4d63-8851-3c391c4cc389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mol_data.edge_index dtype: torch.int64\n",
      "mol_data.edge_index shape: torch.Size([2, 50])\n",
      "pro_data.edge_index dtype: torch.int64\n",
      "pro_data.edge_index shape: torch.Size([2, 5264])\n",
      "Sample 0:\n",
      "  mol_data.edge_index dtype: torch.int64\n",
      "  mol_data.edge_index shape: torch.Size([2, 50])\n",
      "  pro_data.edge_index dtype: torch.int64\n",
      "  pro_data.edge_index shape: torch.Size([2, 5264])\n",
      "Sample 1:\n",
      "  mol_data.edge_index dtype: torch.int64\n",
      "  mol_data.edge_index shape: torch.Size([2, 66])\n",
      "  pro_data.edge_index dtype: torch.int64\n",
      "  pro_data.edge_index shape: torch.Size([2, 2710])\n",
      "Sample 2:\n",
      "  mol_data.edge_index dtype: torch.int64\n",
      "  mol_data.edge_index shape: torch.Size([2, 66])\n",
      "  pro_data.edge_index dtype: torch.int64\n",
      "  pro_data.edge_index shape: torch.Size([2, 3916])\n",
      "Sample 3:\n",
      "  mol_data.edge_index dtype: torch.int64\n",
      "  mol_data.edge_index shape: torch.Size([2, 54])\n",
      "  pro_data.edge_index dtype: torch.int64\n",
      "  pro_data.edge_index shape: torch.Size([2, 5264])\n",
      "Sample 4:\n",
      "  mol_data.edge_index dtype: torch.int64\n",
      "  mol_data.edge_index shape: torch.Size([2, 52])\n",
      "  pro_data.edge_index dtype: torch.int64\n",
      "  pro_data.edge_index shape: torch.Size([2, 2748])\n",
      "Running on cpu.\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Fold 1, Epoch 1/1000, Loss: 1.269109454799945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 246\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  pro_data.edge_index shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpro_data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# Run 5-fold cross-validation training\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 165\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    163\u001b[0m output \u001b[38;5;241m=\u001b[39m model(mol_batch, pro_batch)\n\u001b[1;32m    164\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target)\n\u001b[0;32m--> 165\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    167\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_mse(labels, preds):\n",
    "    return mean_squared_error(labels, preds)\n",
    "\n",
    "def get_pearson(labels, preds):\n",
    "    return pearsonr(labels, preds)[0]\n",
    "\n",
    "def get_ci(labels, preds):\n",
    "    # Concordance Index (CI) implementation\n",
    "    n = 0\n",
    "    h_sum = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] != labels[j]:\n",
    "                n += 1\n",
    "                if (preds[i] < preds[j] and labels[i] < labels[j]) or (preds[i] > preds[j] and labels[i] > labels[j]):\n",
    "                    h_sum += 1\n",
    "                elif preds[i] == preds[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n > 0 else 0.5\n",
    "\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # **Adjust 'edge_index' for mol_data**\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # **Adjust 'edge_index' for pro_data**\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "\n",
    "def batch_loader(sample_files, sample_dir, batch_size):\n",
    "    for i in range(0, len(sample_files), batch_size):\n",
    "        batch_files = sample_files[i:i + batch_size]\n",
    "        batch_samples = []\n",
    "        for file_name in batch_files:\n",
    "            sample_path = os.path.join(sample_dir, file_name)\n",
    "            batch_samples.append(load_sample(sample_path))\n",
    "        yield batch_samples\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)  # Should be 78\n",
    "        num_features_pro = pro_data.x.size(1)  # Should be 52\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Train in batches\n",
    "            for batch_samples in batch_loader(train_files, sample_dir, batch_size=4):\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        model.eval()\n",
    "        total_preds, total_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_samples in batch_loader(test_files, sample_dir, batch_size=4):\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).to(device)\n",
    "\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                total_preds.append(output.cpu().numpy())\n",
    "                total_labels.append(target.cpu().numpy())\n",
    "\n",
    "        # Convert lists to numpy arrays for evaluation\n",
    "        total_preds = np.concatenate(total_preds)\n",
    "        total_labels = np.concatenate(total_labels)\n",
    "\n",
    "        mse = get_mse(total_labels, total_preds)\n",
    "        ci = get_ci(total_labels, total_preds)\n",
    "        pearson = get_pearson(total_labels, total_preds)\n",
    "        print(f\"Fold {fold+1} - MSE: {mse}, CI: {ci}, Pearson: {pearson}\")\n",
    "\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Path to the directory containing prepared individual samples\n",
    "sample_dir = 'prepared_samples'  # Adjust the path to where the .pt files are stored\n",
    "\n",
    "# Place the debugging code here\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your sample directory\n",
    "    sample_dir = 'prepared_samples'  # Adjust this path as needed\n",
    "\n",
    "    # List all sample files\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "\n",
    "    # Test the first sample (or multiple samples)\n",
    "    sample_path = os.path.join(sample_dir, sample_files[0])\n",
    "    mol_data, pro_data, target = load_sample(sample_path)\n",
    "\n",
    "    print(f\"mol_data.edge_index dtype: {mol_data.edge_index.dtype}\")\n",
    "    print(f\"mol_data.edge_index shape: {mol_data.edge_index.shape}\")\n",
    "    print(f\"pro_data.edge_index dtype: {pro_data.edge_index.dtype}\")\n",
    "    print(f\"pro_data.edge_index shape: {pro_data.edge_index.shape}\")\n",
    "\n",
    "    # Optionally, check multiple samples\n",
    "    for i in range(5):  # Adjust the range as needed\n",
    "        sample_path = os.path.join(sample_dir, sample_files[i])\n",
    "        mol_data, pro_data, target = load_sample(sample_path)\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"  mol_data.edge_index dtype: {mol_data.edge_index.dtype}\")\n",
    "        print(f\"  mol_data.edge_index shape: {mol_data.edge_index.shape}\")\n",
    "        print(f\"  pro_data.edge_index dtype: {pro_data.edge_index.dtype}\")\n",
    "        print(f\"  pro_data.edge_index shape: {pro_data.edge_index.shape}\")\n",
    "\n",
    "\n",
    "# Run 5-fold cross-validation training\n",
    "results = train_5fold_cross_validation(sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c5f758-3934-47ab-940f-d8918e56f3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 126\u001b[0m\n\u001b[1;32m    123\u001b[0m sample_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepared_samples\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust the path to where the .pt files are stored\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Run 5-fold cross-validation training\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 80\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_samples \u001b[38;5;129;01min\u001b[39;00m batch_loader(train_files, sample_dir, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m     78\u001b[0m     mol_data_list, pro_data_list, target_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch_samples)\n\u001b[0;32m---> 80\u001b[0m     mol_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_data_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m     pro_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(pro_data_list)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     82\u001b[0m     target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(target_list)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got dict"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def get_mse(labels, preds):\n",
    "    return mean_squared_error(labels, preds)\n",
    "\n",
    "def get_pearson(labels, preds):\n",
    "    return pearsonr(labels, preds)[0]\n",
    "\n",
    "def get_ci(labels, preds):\n",
    "    # Concordance Index (CI) implementation\n",
    "    n = 0\n",
    "    h_sum = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] != labels[j]:\n",
    "                n += 1\n",
    "                if (preds[i] < preds[j] and labels[i] < labels[j]) or (preds[i] > preds[j] and labels[i] > labels[j]):\n",
    "                    h_sum += 1\n",
    "                elif preds[i] == preds[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n > 0 else 0.5\n",
    "\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    return torch.load(path)\n",
    "\n",
    "def batch_loader(sample_indices, sample_dir, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generator to load batches of samples from disk.\n",
    "\n",
    "    Parameters:\n",
    "    sample_indices (list): List of sample indices for the current fold.\n",
    "    sample_dir (str): Directory where individual samples are saved.\n",
    "    batch_size (int): Number of samples per batch.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(sample_indices), batch_size):\n",
    "        batch_files = sample_indices[i:i + batch_size]\n",
    "        batch_samples = []\n",
    "        for file_name in batch_files:\n",
    "            sample_path = os.path.join(sample_dir, file_name)\n",
    "            batch_samples.append(load_sample(sample_path))\n",
    "        yield batch_samples\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Initialize the GNN model\n",
    "        model = GNNNet().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Train in batches\n",
    "            for batch_samples in batch_loader(train_files, sample_dir, batch_size=32):\n",
    "                mol_data_list, pro_data_list, target_list = zip(*batch_samples)\n",
    "\n",
    "                mol_data = torch.stack(mol_data_list).to(device)\n",
    "                pro_data = torch.stack(pro_data_list).to(device)\n",
    "                target = torch.stack(target_list).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_data, pro_data)\n",
    "                loss = loss_fn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_files)}\")\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        model.eval()\n",
    "        total_preds, total_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_samples in batch_loader(test_files, sample_dir, batch_size=32):\n",
    "                mol_data_list, pro_data_list, target_list = zip(*batch_samples)\n",
    "\n",
    "                mol_data = torch.stack(mol_data_list).to(device)\n",
    "                pro_data = torch.stack(pro_data_list).to(device)\n",
    "                target = torch.stack(target_list).to(device)\n",
    "\n",
    "                output = model(mol_data, pro_data)\n",
    "                total_preds.append(output.cpu().numpy())\n",
    "                total_labels.append(target.cpu().numpy())\n",
    "\n",
    "        # Convert lists to numpy arrays for evaluation\n",
    "        total_preds = np.concatenate(total_preds)\n",
    "        total_labels = np.concatenate(total_labels)\n",
    "\n",
    "        mse = get_mse(total_labels, total_preds)\n",
    "        ci = get_ci(total_labels, total_preds)\n",
    "        pearson = get_pearson(total_labels, total_preds)\n",
    "        print(f\"Fold {fold+1} - MSE: {mse}, CI: {ci}, Pearson: {pearson}\")\n",
    "\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Path to the directory containing prepared individual samples\n",
    "sample_dir = 'prepared_samples'  # Adjust the path to where the .pt files are stored\n",
    "\n",
    "# Run 5-fold cross-validation training\n",
    "results = train_5fold_cross_validation(sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c3b169-eaff-4415-a108-a79609835cfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'prepared_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Load the prepared dataset\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m prepared_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprepared_samples\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the path if you saved each sample individually\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Run 5-fold cross-validation training with the pre-prepared dataset\u001b[39;00m\n\u001b[1;32m    106\u001b[0m results \u001b[38;5;241m=\u001b[39m train_5fold_cross_validation(prepared_dataset)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'prepared_samples'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_mse(labels, preds):\n",
    "    return mean_squared_error(labels, preds)\n",
    "\n",
    "def get_pearson(labels, preds):\n",
    "    return pearsonr(labels, preds)[0]\n",
    "\n",
    "def get_ci(labels, preds):\n",
    "    # Concordance Index (CI) implementation\n",
    "    n = 0\n",
    "    h_sum = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] != labels[j]:\n",
    "                n += 1\n",
    "                if (preds[i] < preds[j] and labels[i] < labels[j]) or (preds[i] > preds[j] and labels[i] > labels[j]):\n",
    "                    h_sum += 1\n",
    "                elif preds[i] == preds[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n > 0 else 0.5\n",
    "\n",
    "def train_5fold_cross_validation(prepared_dataset, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(prepared_dataset)):\n",
    "        print(f'Fold {fold + 1}/{n_splits}')\n",
    "        \n",
    "        # Split the dataset into training and testing based on indices\n",
    "        train_data = [prepared_dataset[i] for i in train_idx]\n",
    "        test_data = [prepared_dataset[i] for i in test_idx]\n",
    "        \n",
    "        # Create DataLoader for training and testing\n",
    "        train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Initialize the GNN model\n",
    "        model = GNNNet().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_idx, (mol_data, pro_data, target) in enumerate(train_loader):\n",
    "                mol_data = mol_data.to(device)\n",
    "                pro_data = pro_data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_data, pro_data)\n",
    "                loss = loss_fn(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "            \n",
    "            # Save model checkpoints after every few epochs if needed\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                torch.save(model.state_dict(), f\"model_checkpoint_fold{fold+1}_epoch{epoch+1}.pt\")\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        model.eval()\n",
    "        total_preds, total_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for mol_data, pro_data, target in test_loader:\n",
    "                mol_data = mol_data.to(device)\n",
    "                pro_data = pro_data.to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(mol_data, pro_data)\n",
    "                total_preds.append(output.cpu().numpy())\n",
    "                total_labels.append(target.cpu().numpy())\n",
    "\n",
    "        # Convert lists to numpy arrays for evaluation\n",
    "        total_preds = np.concatenate(total_preds)\n",
    "        total_labels = np.concatenate(total_labels)\n",
    "\n",
    "        mse = get_mse(total_labels, total_preds)\n",
    "        ci = get_ci(total_labels, total_preds)\n",
    "        pearson = get_pearson(total_labels, total_preds)\n",
    "        print(f\"Fold {fold+1} - MSE: {mse}, CI: {ci}, Pearson: {pearson}\")\n",
    "\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Load the prepared dataset\n",
    "prepared_dataset = torch.load('prepared_samples')  # Adjust the path if you saved each sample individually\n",
    "\n",
    "# Run 5-fold cross-validation training with the pre-prepared dataset\n",
    "results = train_5fold_cross_validation(prepared_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4f8409-e1b8-4a09-ab46-35224976ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_mse(labels, preds):\n",
    "    return mean_squared_error(labels, preds)\n",
    "\n",
    "def get_pearson(labels, preds):\n",
    "    return pearsonr(labels, preds)[0]\n",
    "\n",
    "def get_ci(labels, preds):\n",
    "    # Concordance Index (CI) implementation\n",
    "    n = 0\n",
    "    h_sum = 0\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            if labels[i] != labels[j]:\n",
    "                n += 1\n",
    "                if (preds[i] < preds[j] and labels[i] < labels[j]) or (preds[i] > preds[j] and labels[i] > labels[j]):\n",
    "                    h_sum += 1\n",
    "                elif preds[i] == preds[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n > 0 else 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73eb7c92-85c0-44fd-83e6-0247c3980995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "def train_5fold_cross_validation(data, molecule_graphs, protein_graphs, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Print if the model is on GPU or CPU\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Model is running on GPU.\")\n",
    "    else:\n",
    "        print(\"Model is running on CPU.\")\n",
    "\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    # Prepare dataset and dataloaders\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(data)):\n",
    "        print(f'Fold {fold + 1}/{n_splits}')\n",
    "        \n",
    "        train_data = data.iloc[train_idx]\n",
    "        test_data = data.iloc[test_idx]\n",
    "        \n",
    "        train_loader = DataLoader(prepare_dataset(train_data, molecule_graphs, protein_graphs), batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(prepare_dataset(test_data, molecule_graphs, protein_graphs), batch_size=32, shuffle=False)\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model = GNNNet().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in train_loader:\n",
    "                mol_data, pro_data, target = batch\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_data.to(device), pro_data.to(device))\n",
    "                loss = loss_fn(output, target.to(device))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item()}\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        total_preds, total_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                mol_data, pro_data, target = batch\n",
    "                output = model(mol_data.to(device), pro_data.to(device))\n",
    "                total_preds.append(output.cpu().numpy())\n",
    "                total_labels.append(target.cpu().numpy())\n",
    "\n",
    "        mse = get_mse(total_labels, total_preds)\n",
    "        ci = get_ci(total_labels, total_preds)\n",
    "        pearson = get_pearson(total_labels, total_preds)\n",
    "        print(f\"Fold {fold+1} - MSE: {mse}, CI: {ci}, Pearson: {pearson}\")\n",
    "\n",
    "        # Save the results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a1d8e4-aa58-4187-b1ac-c2864d446d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def load_graph(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    print(\"graph is loaded \")\n",
    "\n",
    "def prepare_dataset(data, molecule_graphs, protein_graphs):\n",
    "    dataset = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        mol_graph_path = os.path.join(molecule_graphs, f\"{row['Drug_ID']}_graph.pkl\")\n",
    "        pro_graph_path = os.path.join(protein_graphs, f\"{row['Target_ID']}_graph.pt\")\n",
    "        \n",
    "        mol_graph = load_graph(mol_graph_path)\n",
    "        pro_graph = torch.load(pro_graph_path)\n",
    "        target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "        \n",
    "        dataset.append((mol_graph, pro_graph, target))\n",
    "\n",
    "    print(\"Dataset is ready\")\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653fc68-0ac0-4e6e-a169-e3d027dc7c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is running on GPU.\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36879/4181204454.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pro_graph = torch.load(pro_graph_path)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "molecule_graphs = 'molecule_graphs/'\n",
    "protein_graphs = 'ProteinGraphs/'\n",
    "filtered_dataset_path = 'filtered_KibaDataSet.csv'\n",
    "\n",
    "# Load filtered dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# Run 5-fold cross-validation training\n",
    "results = train_5fold_cross_validation(data, molecule_graphs, protein_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0cdad-5cd3-443b-85dc-499320b6c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
