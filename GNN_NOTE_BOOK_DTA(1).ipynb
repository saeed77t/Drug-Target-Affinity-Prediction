{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1t21jwkjEYb",
    "outputId": "ac1ec146-dcd4-4a90-a3f0-16fcb3eece86"
   },
   "outputs": [],
   "source": [
    "# !unzip /content/full\\ graphs.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfeeBFxj-LQf",
    "outputId": "55756045-3e2f-45e9-8245-9b3a77d52436"
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n",
    "# import os\n",
    "# import torch\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_graph(path, is_pickle=True):\n",
    "#     \"\"\"\n",
    "#     Load a molecule graph (.pkl) or a protein graph (.pt).\n",
    "#     If is_pickle is True, use pickle to load the file; otherwise, use torch.load.\n",
    "#     \"\"\"\n",
    "#     if is_pickle:\n",
    "#         with open(path, 'rb') as f:\n",
    "#             return pickle.load(f)\n",
    "#     else:\n",
    "#         return torch.load(path)\n",
    "\n",
    "# def prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir):\n",
    "#     \"\"\"\n",
    "#     Incrementally prepares the dataset and saves each (molecule, protein, target) tuple as a separate .pt file.\n",
    "\n",
    "#     Args:\n",
    "#     - filtered_dataset: The filtered KIBA dataset (DataFrame).\n",
    "#     - molecule_graph_dir: Directory where molecule graphs are stored.\n",
    "#     - protein_graph_dir: Directory where protein graphs are stored.\n",
    "#     - output_dir: Directory to save the prepared dataset incrementally.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     for index, row in filtered_dataset.iterrows():\n",
    "#         protein_id = row['Target_ID']\n",
    "#         chembl_id = row['Drug_ID']\n",
    "\n",
    "#         # Load the protein graph (.pt)\n",
    "#         pro_graph_path = os.path.join(protein_graph_dir, f\"{protein_id}_graph.pt\")\n",
    "#         if not os.path.exists(pro_graph_path):\n",
    "#             print(f\"Protein graph not found: {protein_id}\")\n",
    "#             continue\n",
    "#         pro_graph = load_graph(pro_graph_path, is_pickle=False)\n",
    "\n",
    "#         # Load the molecule graph (.pkl)\n",
    "#         mol_graph_path = os.path.join(molecule_graph_dir, f\"{chembl_id}_graph.pkl\")\n",
    "#         if not os.path.exists(mol_graph_path):\n",
    "#             print(f\"Molecule graph not found: {chembl_id}\")\n",
    "#             continue\n",
    "#         mol_graph = load_graph(mol_graph_path)\n",
    "\n",
    "#         # Load target (affinity value)\n",
    "#         target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "\n",
    "#         # Create the sample as a tuple (molecule graph, protein graph, target)\n",
    "#         sample = (mol_graph, pro_graph, target)\n",
    "\n",
    "#         # Save the sample as a .pt file\n",
    "#         sample_path = os.path.join(output_dir, f\"sample_{index}.pt\")\n",
    "#         torch.save(sample, sample_path)\n",
    "\n",
    "#         print(f\"Saved sample {index} as {sample_path}\")\n",
    "\n",
    "# # Example usage for individual saving\n",
    "# molecule_graph_dir = '/content/molecule_graphs'  # Directory where molecule graphs are stored\n",
    "# protein_graph_dir = '/content/ProteinGraphs'  # Directory where protein graphs are stored\n",
    "# filtered_dataset_path = '/content/filtered_KibaDataSet.csv'  # Path to the filtered dataset CSV\n",
    "# output_dir = '/content/prepared_samples/'  # Directory to save individual samples\n",
    "\n",
    "# # Load filtered dataset CSV\n",
    "# filtered_dataset = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# # Prepare the dataset incrementally, saving each sample as a .pt file\n",
    "# prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir)\n",
    "\n",
    "# print(\"Dataset preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nPXE88wN8hYi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_max_pool as gmp, global_add_pool as gap,global_mean_pool as gep,global_sort_pool\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "\n",
    "# GCN based model\n",
    "class GNNNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro=54, num_features_mol=78, output_dim=128, dropout=0.2):\n",
    "        super(GNNNet, self).__init__()\n",
    "\n",
    "        print('GNNNet Loaded')\n",
    "        self.n_output = n_output\n",
    "        self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "        self.mol_conv2 = GCNConv(num_features_mol, num_features_mol * 2)\n",
    "        self.mol_conv3 = GCNConv(num_features_mol * 2, num_features_mol * 4)\n",
    "        self.mol_fc_g1 = torch.nn.Linear(num_features_mol * 4, 1024)\n",
    "        self.mol_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        # self.pro_conv1 = GCNConv(embed_dim, embed_dim)\n",
    "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro_conv2 = GCNConv(num_features_pro, num_features_pro * 2)\n",
    "        self.pro_conv3 = GCNConv(num_features_pro * 2, num_features_pro * 4)\n",
    "        # self.pro_conv4 = GCNConv(embed_dim * 4, embed_dim * 8)\n",
    "        self.pro_fc_g1 = torch.nn.Linear(num_features_pro * 4, 1024)\n",
    "        self.pro_fc_g2 = torch.nn.Linear(1024, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # combined layers\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        # get graph input\n",
    "        mol_x, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "        # get protein input\n",
    "        target_x, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "\n",
    "        # target_seq=data_pro.target\n",
    "\n",
    "        # print('size')\n",
    "        # print('mol_x', mol_x.size(), 'edge_index', mol_edge_index.size(), 'batch', mol_batch.size())\n",
    "        # print('target_x', target_x.size(), 'target_edge_index', target_batch.size(), 'batch', target_batch.size())\n",
    "\n",
    "        x = self.mol_conv1(mol_x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv2(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # mol_edge_index, _ = dropout_adj(mol_edge_index, training=self.training)\n",
    "        x = self.mol_conv3(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = gep(x, mol_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        x = self.relu(self.mol_fc_g1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_fc_g2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        xt = self.pro_conv1(target_x, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv2(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\n",
    "        xt = self.pro_conv3(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "\n",
    "        # xt = self.pro_conv4(xt, target_edge_index)\n",
    "        # xt = self.relu(xt)\n",
    "        xt = gep(xt, target_batch)  # global pooling\n",
    "\n",
    "        # flatten\n",
    "        xt = self.relu(self.pro_fc_g1(xt))\n",
    "        xt = self.dropout(xt)\n",
    "        xt = self.pro_fc_g2(xt)\n",
    "        xt = self.dropout(xt)\n",
    "\n",
    "        # print(x.size(), xt.size())\n",
    "        # concat\n",
    "        xc = torch.cat((x, xt), 1)\n",
    "        # add some dense layers\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "No checkpoint found for fold 1, loading pretrained model from 'model_GNNNet_kiba.model'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   0%|          | 0/5 [15:42<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/5, Loss: 10.3111\n",
      "Checkpoint saved for fold 1 at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 1/5 [23:32<1:34:09, 1412.46s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/5 - MSE: 0.6774, CI: 0.4904, Pearson: -0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 1/5 [40:22<1:34:09, 1412.46s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/5, Loss: 0.7066\n",
      "Checkpoint saved for fold 1 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|████      | 2/5 [48:11<1:12:34, 1451.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/5 - MSE: 0.6781, CI: 0.5134, Pearson: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|████      | 2/5 [1:04:27<1:12:34, 1451.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/5, Loss: 0.6967\n",
      "Checkpoint saved for fold 1 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  60%|██████    | 3/5 [1:12:24<48:24, 1452.43s/epoch]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/5 - MSE: 0.6842, CI: 0.5306, Pearson: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  60%|██████    | 3/5 [1:29:43<48:24, 1452.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/5, Loss: 0.6910\n",
      "Checkpoint saved for fold 1 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  80%|████████  | 4/5 [1:37:16<24:27, 1467.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/5 - MSE: 0.6788, CI: 0.5495, Pearson: 0.1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  80%|████████  | 4/5 [1:48:29<27:07, 1627.32s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 227\u001b[0m\n\u001b[1;32m    224\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 151\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    148\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(target_list, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    150\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 151\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpro_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target)\n\u001b[1;32m    153\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 71\u001b[0m, in \u001b[0;36mGNNNet.forward\u001b[0;34m(self, data_mol, data_pro)\u001b[0m\n\u001b[1;32m     68\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(xt)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpro_conv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_edge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(xt)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# target_edge_index, _ = dropout_adj(target_edge_index, training=self.training)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/var/folders/gv/29bx0dbj5fvf2ql1sq4xwxsh0000gn/T/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_xyro4j5d.py:230\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_weight, size)\u001b[0m\n\u001b[1;32m    221\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    222\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    223\u001b[0m                 edge_weight\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39medge_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    227\u001b[0m             )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:625\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    610\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    615\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:128\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py:182\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ... [Your existing imports and GNNNet class definition remain unchanged] ...\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def adjust_state_dict(state_dict, model_state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k in state_dict.keys():\n",
    "        new_k = k\n",
    "        # Adjust mol_conv layers\n",
    "        if 'mol_conv' in k:\n",
    "            if 'weight' in k and 'lin.weight' not in k:\n",
    "                new_k = k.replace('weight', 'lin.weight')\n",
    "            if 'bias' in k and 'lin.bias' not in k:\n",
    "                new_k = k.replace('bias', 'lin.bias')\n",
    "        # Adjust pro_conv layers\n",
    "        elif 'pro_conv' in k:\n",
    "            if 'weight' in k and 'lin.weight' not in k:\n",
    "                new_k = k.replace('weight', 'lin.weight')\n",
    "            if 'bias' in k and 'lin.bias' not in k:\n",
    "                new_k = k.replace('bias', 'lin.bias')\n",
    "\n",
    "        param = state_dict[k]\n",
    "        if new_k in model_state_dict:\n",
    "            model_param = model_state_dict[new_k]\n",
    "            if param.shape != model_param.shape:\n",
    "                if len(param.shape) == 2:\n",
    "                    # Transpose the weight matrix\n",
    "                    param = param.t()\n",
    "                    if param.shape != model_param.shape:\n",
    "                        print(f\"Shape mismatch after transpose for {new_k}: checkpoint {param.shape}, model {model_param.shape}\")\n",
    "                else:\n",
    "                    print(f\"Shape mismatch for {new_k}: checkpoint {param.shape}, model {model_param.shape}\")\n",
    "        new_state_dict[new_k] = param\n",
    "    return new_state_dict\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, loading pretrained model from 'model_GNNNet_kiba.model'.\")\n",
    "\n",
    "            # Load the pretrained model weights and adjust the keys\n",
    "            state_dict = torch.load('model_GNNNet_kiba.model', map_location=device)\n",
    "            adjusted_state_dict = adjust_state_dict(state_dict, model.state_dict())\n",
    "            model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "\n",
    "            # The optimizer remains as initialized\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 256 # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 256  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 5  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (NoneType, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 200\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# Optionally, plot the results\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# plot_density(Y, P)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 144\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    143\u001b[0m sample_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sample_dir, sample_files[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 144\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mload_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 88\u001b[0m, in \u001b[0;36mload_sample\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Ensure that 'batch' attribute is set\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(mol_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m mol_data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     mol_data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pro_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m pro_data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     pro_data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(pro_data\u001b[38;5;241m.\u001b[39mnum_nodes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (NoneType, dtype=torch.dtype), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Define metric functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    n = 0\n",
    "    h_sum = 0.0\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true)):\n",
    "            if y_true[i] > y_true[j]:\n",
    "                n += 1\n",
    "                if y_pred[i] > y_pred[j]:\n",
    "                    h_sum += 1\n",
    "                elif y_pred[i] == y_pred[j]:\n",
    "                    h_sum += 0.5\n",
    "    return h_sum / n if n != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "def get_spearman(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred)[0]\n",
    "\n",
    "def get_rmse(y_true, y_pred):\n",
    "    return np.sqrt(get_mse(y_true, y_pred))\n",
    "\n",
    "def get_rm2(y_true, y_pred):\n",
    "    y_mean = np.mean(y_true)\n",
    "    ss_tot = np.sum((y_true - y_mean) ** 2)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    return r_squared\n",
    "\n",
    "import torch\n",
    "\n",
    "def adjust_state_dict(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k in state_dict.keys():\n",
    "        new_k = k\n",
    "        # Adjust mol_conv layers\n",
    "        if 'mol_conv' in k:\n",
    "            if 'lin.weight' in k:\n",
    "                new_k = k.replace('lin.weight', 'weight')\n",
    "            elif 'lin.bias' in k:\n",
    "                new_k = k.replace('lin.bias', 'bias')\n",
    "        # Adjust pro_conv layers\n",
    "        elif 'pro_conv' in k:\n",
    "            if 'lin.weight' in k:\n",
    "                new_k = k.replace('lin.weight', 'weight')\n",
    "            elif 'lin.bias' in k:\n",
    "                new_k = k.replace('lin.bias', 'bias')\n",
    "        new_state_dict[new_k] = state_dict[k]\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    data = torch.load(path)\n",
    "    if isinstance(data, tuple) and len(data) == 3:\n",
    "        mol_data, pro_data, target = data\n",
    "    elif isinstance(data, dict):\n",
    "        mol_data = data.get('mol_data')\n",
    "        pro_data = data.get('pro_data')\n",
    "        target = data.get('target')\n",
    "    else:\n",
    "        print(f\"Unexpected data format in file {path}\")\n",
    "        return None\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'batch' attribute is set\n",
    "    if not hasattr(mol_data, 'batch') or mol_data.batch is None:\n",
    "        mol_data.batch = torch.zeros(mol_data.num_nodes, dtype=torch.long)\n",
    "    if not hasattr(pro_data, 'batch') or pro_data.batch is None:\n",
    "        pro_data.batch = torch.zeros(pro_data.num_nodes, dtype=torch.long)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the predicting function\n",
    "def predicting(model, device, samples):\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    print('Making predictions for {} samples...'.format(len(samples)))\n",
    "    with torch.no_grad():\n",
    "        batch_size = 256  # Adjust as necessary\n",
    "        for i in tqdm(range(0, len(samples), batch_size), desc='Predicting'):\n",
    "            batch_samples = samples[i:i+batch_size]\n",
    "            mol_data_list = []\n",
    "            pro_data_list = []\n",
    "            target_list = []\n",
    "            for sample in batch_samples:\n",
    "                if sample is None:\n",
    "                    continue\n",
    "                mol_data, pro_data, target = sample\n",
    "                mol_data_list.append(mol_data)\n",
    "                pro_data_list.append(pro_data)\n",
    "                target_list.append(target)\n",
    "            if len(mol_data_list) == 0:\n",
    "                continue\n",
    "            mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "            pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "            targets = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "            outputs = model(mol_batch, pro_batch)\n",
    "            total_preds.append(outputs.cpu().numpy())\n",
    "            total_labels.append(targets.cpu().numpy())\n",
    "    if len(total_preds) == 0:\n",
    "        print(\"No predictions were made.\")\n",
    "        return None, None\n",
    "    total_preds = np.concatenate(total_preds)\n",
    "    total_labels = np.concatenate(total_labels)\n",
    "    return total_labels, total_preds\n",
    "\n",
    "# Define the main function\n",
    "def main():\n",
    "    # Set the sample directory where your data is stored\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your available samples\n",
    "    model_path = 'model_GNNNet_kiba.model'  # Path to your pretrained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    \n",
    "    # Load the model\n",
    "    # Determine input feature dimensions from your data\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    if not sample_files:\n",
    "        print(f\"No sample files found in {sample_dir}\")\n",
    "        return\n",
    "    sample_file_path = os.path.join(sample_dir, sample_files[0])\n",
    "    sample = load_sample(sample_file_path)\n",
    "    if sample is None:\n",
    "        print(\"Failed to load sample.\")\n",
    "        return\n",
    "\n",
    "    mol_data, pro_data, _ = sample\n",
    "\n",
    "    num_features_mol = mol_data.x.size(1)\n",
    "    num_features_pro = pro_data.x.size(1)\n",
    "    \n",
    "    model = GNNNet(num_features_mol=num_features_mol, num_features_pro=num_features_pro).to(device)\n",
    "    \n",
    "    \n",
    "    # Load the pretrained model weights and adjust the keys\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    adjusted_state_dict = adjust_state_dict(state_dict, model.state_dict())\n",
    "    model.load_state_dict(adjusted_state_dict, strict=False)\n",
    "    \n",
    "    # Load the data\n",
    "    samples = []\n",
    "    for sample_file in sample_files:\n",
    "        sample_path = os.path.join(sample_dir, sample_file)\n",
    "        sample = load_sample(sample_path)\n",
    "        if sample is not None:\n",
    "            samples.append(sample)\n",
    "    if not samples:\n",
    "        print(f\"No valid samples found in {sample_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Run predictions\n",
    "    Y, P = predicting(model, device, samples)\n",
    "    if Y is None or P is None:\n",
    "        print(\"No predictions were made.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = get_mse(Y, P)\n",
    "    ci = get_ci(Y, P)\n",
    "    pearson = get_pearson(Y, P)\n",
    "    spearman = get_spearman(Y, P)\n",
    "    rmse = get_rmse(Y, P)\n",
    "    rm2 = get_rm2(Y, P)\n",
    "    \n",
    "    # Print metrics\n",
    "    print('Evaluation Metrics:')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'Pearson Correlation: {pearson:.4f}')\n",
    "    print(f'Spearman Correlation: {spearman:.4f}')\n",
    "    print(f'CI: {ci:.4f}')\n",
    "    print(f'R-squared (RM2): {rm2:.4f}')\n",
    "    \n",
    "    # Optionally, plot the results\n",
    "    # plot_density(Y, P)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# from torch.nn import MSELoss\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from torch_geometric.data import Data, Batch\n",
    "# from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "# from scipy.stats import pearsonr\n",
    "# import warnings\n",
    "# import itertools\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # Suppress FutureWarning related to torch.load\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# # Define the load_sample function\n",
    "# def load_sample(path):\n",
    "#     # Load individual sample from file\n",
    "#     sample = torch.load(path)\n",
    "#     mol_data = sample[0]\n",
    "#     pro_data = sample[1]\n",
    "#     target = sample[2]\n",
    "\n",
    "#     # Convert dictionaries to Data objects if necessary\n",
    "#     if isinstance(mol_data, dict):\n",
    "#         mol_data = Data(**mol_data)\n",
    "#     if isinstance(pro_data, dict):\n",
    "#         pro_data = Data(**pro_data)\n",
    "\n",
    "#     # Ensure that 'x' attribute is set\n",
    "#     if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "#         if hasattr(mol_data, 'features'):\n",
    "#             mol_data.x = mol_data.features\n",
    "#             del mol_data.features\n",
    "#         else:\n",
    "#             raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "#     if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "#         if hasattr(pro_data, 'features'):\n",
    "#             pro_data.x = pro_data.features\n",
    "#             del pro_data.features\n",
    "#         else:\n",
    "#             raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "#     # Ensure 'x' is a float tensor\n",
    "#     if not isinstance(mol_data.x, torch.Tensor):\n",
    "#         mol_data.x = torch.tensor(mol_data.x)\n",
    "#     if not isinstance(pro_data.x, torch.Tensor):\n",
    "#         pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "#     if mol_data.x.dtype != torch.float:\n",
    "#         mol_data.x = mol_data.x.float()\n",
    "#     if pro_data.x.dtype != torch.float:\n",
    "#         pro_data.x = pro_data.x.float()\n",
    "\n",
    "#     # Adjust 'edge_index' for mol_data\n",
    "#     if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "#         mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "#     else:\n",
    "#         mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "#     if mol_data.edge_index.shape[0] != 2:\n",
    "#         mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "#     # Adjust 'edge_index' for pro_data\n",
    "#     if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "#         pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "#     else:\n",
    "#         pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "#     if pro_data.edge_index.shape[0] != 2:\n",
    "#         pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "#     # Set 'num_nodes' attribute to suppress warnings\n",
    "#     mol_data.num_nodes = mol_data.x.size(0)\n",
    "#     pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "#     return (mol_data, pro_data, target)\n",
    "\n",
    "# # Define the batch_loader function\n",
    "# def batch_loader(file_list, sample_dir, batch_size):\n",
    "#     batch = []\n",
    "#     for idx, file_name in enumerate(file_list):\n",
    "#         sample_path = os.path.join(sample_dir, file_name)\n",
    "#         sample = load_sample(sample_path)\n",
    "#         batch.append(sample)\n",
    "#         if len(batch) == batch_size:\n",
    "#             yield batch\n",
    "#             batch = []\n",
    "#     if len(batch) > 0:\n",
    "#         yield batch\n",
    "\n",
    "# # Define the evaluation metrics functions\n",
    "# def get_mse(y_true, y_pred):\n",
    "#     return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# def get_ci(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Compute the concordance index between true and predicted values.\n",
    "#     \"\"\"\n",
    "#     pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "#     c = 0\n",
    "#     s = 0\n",
    "#     for i, j in pairs:\n",
    "#         if y_true[i] != y_true[j]:\n",
    "#             s += 1\n",
    "#             if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "#                (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "#                 c += 1\n",
    "#             elif y_pred[i] == y_pred[j]:\n",
    "#                 c += 0.5\n",
    "#     return c / s if s != 0 else 0\n",
    "\n",
    "# def get_pearson(y_true, y_pred):\n",
    "#     return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "# def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Running on {device}.\")\n",
    "\n",
    "#     sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "#     kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Create a single directory for all checkpoints\n",
    "#     training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "#     if not os.path.exists(training_model_dir):\n",
    "#         os.makedirs(training_model_dir)\n",
    "#         print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "#     else:\n",
    "#         print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "#     results = []\n",
    "#     loss_fn = MSELoss()\n",
    "\n",
    "#     for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "#         fold_number = fold + 1\n",
    "#         print(f'\\nFold {fold_number}/{n_splits}')\n",
    "#         train_files = [sample_files[i] for i in train_idx]\n",
    "#         test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "#         # Determine input feature dimensions from your data\n",
    "#         sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "#         mol_data = sample[0]\n",
    "#         pro_data = sample[1]\n",
    "\n",
    "#         num_features_mol = 78\n",
    "#         num_features_pro = 54\n",
    "\n",
    "#         # Initialize the GNN model with correct input dimensions\n",
    "#         model = GNNNet(\n",
    "#             num_features_mol=num_features_mol,\n",
    "#             num_features_pro=num_features_pro\n",
    "#         ).to(device)\n",
    "#         print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "#         # Initialize starting epoch\n",
    "#         start_epoch = 1\n",
    "\n",
    "#         # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "#         existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "#                                 if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "#         if existing_checkpoints:\n",
    "#             # Find the latest checkpoint based on epoch number\n",
    "#             latest_checkpoint = max(existing_checkpoints,\n",
    "#                                     key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "#             checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "#             print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "#             checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "#             # Determine the format of the checkpoint and load accordingly\n",
    "#             if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "#                 # Standard checkpoint format\n",
    "#                 model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#                 optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#                 optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#                 loaded_epoch = checkpoint['epoch']\n",
    "#                 start_epoch = loaded_epoch + 1\n",
    "#                 print(f\"Resuming training from epoch {start_epoch}\")\n",
    "#             else:\n",
    "#                 # Checkpoint is the model's state_dict directly\n",
    "#                 model.load_state_dict(checkpoint)\n",
    "#                 optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#                 # Since epoch is not saved, start from 1 or set as needed\n",
    "#                 start_epoch = 1\n",
    "#                 print(\"Loaded model state_dict directly from checkpoint.\")\n",
    "#                 print(f\"Starting training from epoch {start_epoch}\")\n",
    "#         else:\n",
    "#             print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "#             # Load pre-trained model weights\n",
    "#             model_weight_path = 'model_GNNNet_kiba.model'  # Adjust the path accordingly\n",
    "#             if os.path.exists(model_weight_path):\n",
    "#                 # Load the state_dict from the file\n",
    "#                 state_dict = torch.load(model_weight_path, map_location=device)\n",
    "#                 # Load the state_dict into the model\n",
    "#                 if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "#                     # If state_dict has 'model_state_dict' key, use it\n",
    "#                     model.load_state_dict(state_dict['model_state_dict'])\n",
    "#                     print(f\"Loaded pre-trained model weights from {model_weight_path} using 'model_state_dict' key.\")\n",
    "#                 else:\n",
    "#                     # Assume state_dict is the model's state_dict directly\n",
    "#                     model.load_state_dict(state_dict)\n",
    "#                     print(f\"Loaded pre-trained model weights from {model_weight_path}\")\n",
    "#             else:\n",
    "#                 print(f\"Pre-trained model weights not found at {model_weight_path}, proceeding without loading weights.\")\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#             start_epoch = 1\n",
    "\n",
    "#         # Training loop with progress bar over epochs\n",
    "#         for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "#                           desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "#             model.train()\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#             # Prepare batch loader without progress bar for batches\n",
    "#             batch_size = 128  # Adjust batch size as needed\n",
    "#             batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "#             for batch_samples in batch_loader_iter:\n",
    "#                 mol_data_list = []\n",
    "#                 pro_data_list = []\n",
    "#                 target_list = []\n",
    "\n",
    "#                 for sample in batch_samples:\n",
    "#                     mol_data = sample[0]\n",
    "#                     pro_data = sample[1]\n",
    "#                     target = sample[2]\n",
    "\n",
    "#                     mol_data_list.append(mol_data)\n",
    "#                     pro_data_list.append(pro_data)\n",
    "#                     target_list.append(target)\n",
    "\n",
    "#                 mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "#                 pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "#                 target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 output = model(mol_batch, pro_batch)\n",
    "#                 loss = loss_fn(output.view(-1), target)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "#             avg_loss = running_loss / len(train_files)\n",
    "#             # Use tqdm.write() to print without interfering with the progress bar\n",
    "#             tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#             # Save the model and optimizer states after each epoch\n",
    "#             checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "#             checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             }, checkpoint_path)\n",
    "#             tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "#             # Evaluation on the test set after each epoch\n",
    "#             model.eval()\n",
    "#             total_preds, total_labels = [], []\n",
    "#             with torch.no_grad():\n",
    "#                 batch_size = 128  # Adjust batch size as needed\n",
    "#                 batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "#                 for batch_samples in batch_loader_iter:\n",
    "#                     mol_data_list = []\n",
    "#                     pro_data_list = []\n",
    "#                     target_list = []\n",
    "\n",
    "#                     for sample in batch_samples:\n",
    "#                         mol_data = sample[0]\n",
    "#                         pro_data = sample[1]\n",
    "#                         target = sample[2]\n",
    "\n",
    "#                         mol_data_list.append(mol_data)\n",
    "#                         pro_data_list.append(pro_data)\n",
    "#                         target_list.append(target)\n",
    "\n",
    "#                     mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "#                     pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "#                     target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "#                     output = model(mol_batch, pro_batch)\n",
    "#                     total_preds.append(output.cpu().numpy())\n",
    "#                     total_labels.append(target.cpu().numpy())\n",
    "\n",
    "#             # Convert lists to numpy arrays for evaluation\n",
    "#             total_preds = np.concatenate(total_preds)\n",
    "#             total_labels = np.concatenate(total_labels)\n",
    "\n",
    "#             # Calculate metrics\n",
    "#             mse = get_mse(total_labels, total_preds)\n",
    "#             ci = get_ci(total_labels, total_preds)\n",
    "#             pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "#             # Print metrics\n",
    "#             tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "#         # Evaluation at the end of training for this fold\n",
    "#         print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "#         # Store results for this fold\n",
    "#         results.append((mse, ci, pearson))\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "#     num_epochs = 5  # Adjust the number of epochs as needed\n",
    "#     n_splits = 5  # Number of folds for cross-validation\n",
    "#     learning_rate = 0.001  # Learning rate\n",
    "\n",
    "#     # Run the training function\n",
    "#     results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "#     # Print overall results\n",
    "#     print(\"\\nCross-validation Results:\")\n",
    "#     for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "#         print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "#     # Optionally, compute and print average metrics across folds\n",
    "#     mse_values, ci_values, pearson_values = zip(*results)\n",
    "#     print(f\"\\nAverage Results:\")\n",
    "#     print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "#     print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "#     print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "2Zo3elfYxiQ1",
    "outputId": "be01f5d7-8330-45e2-f7be-9839e82cee19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "Loading checkpoint for fold 1 from prepared_samples/TrainingModel/model_fold1_epoch2.pt\n",
      "Resuming training from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   0%|          | 0/98 [11:41<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/100, Loss: 1.1182\n",
      "Checkpoint saved for fold 1 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/98 [18:24<29:45:14, 1104.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/100 - MSE: 1.2433, CI: 0.6028, Pearson: 0.3476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/98 [30:24<29:45:14, 1104.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/100, Loss: 1.0650\n",
      "Checkpoint saved for fold 1 at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/98 [37:03<29:41:10, 1113.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/100 - MSE: 1.3993, CI: 0.6241, Pearson: 0.3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/98 [49:06<29:41:10, 1113.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5/100, Loss: 0.9468\n",
      "Checkpoint saved for fold 1 at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/98 [55:42<29:26:32, 1115.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5/100 - MSE: 0.8051, CI: 0.6388, Pearson: 0.4455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/98 [1:07:46<29:26:32, 1115.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6/100, Loss: 0.8229\n",
      "Checkpoint saved for fold 1 at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 4/98 [1:14:24<29:11:41, 1118.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6/100 - MSE: 0.7991, CI: 0.6561, Pearson: 0.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 4/98 [1:27:07<29:11:41, 1118.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7/100, Loss: 0.7494\n",
      "Checkpoint saved for fold 1 at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 5/98 [1:33:43<29:16:08, 1132.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7/100 - MSE: 0.8292, CI: 0.6685, Pearson: 0.5049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 5/98 [1:45:55<29:16:08, 1132.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 8/100, Loss: 0.7194\n",
      "Checkpoint saved for fold 1 at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 6/98 [1:52:27<28:52:26, 1129.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 8/100 - MSE: 0.9032, CI: 0.6839, Pearson: 0.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 6/98 [2:04:41<28:52:26, 1129.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 9/100, Loss: 0.6964\n",
      "Checkpoint saved for fold 1 at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 7/98 [2:11:17<28:33:45, 1129.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 9/100 - MSE: 0.9257, CI: 0.6988, Pearson: 0.5519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 7/98 [2:23:04<28:33:45, 1129.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 10/100, Loss: 0.6750\n",
      "Checkpoint saved for fold 1 at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 8/98 [2:29:34<27:59:10, 1119.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 10/100 - MSE: 0.9027, CI: 0.7090, Pearson: 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 8/98 [2:41:43<27:59:10, 1119.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 11/100, Loss: 0.6586\n",
      "Checkpoint saved for fold 1 at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 9/98 [2:48:11<27:39:25, 1118.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 11/100 - MSE: 0.9542, CI: 0.7251, Pearson: 0.5881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 9/98 [3:00:18<27:39:25, 1118.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 12/100, Loss: 0.6408\n",
      "Checkpoint saved for fold 1 at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 10/98 [3:06:44<27:17:59, 1116.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 12/100 - MSE: 0.9246, CI: 0.7334, Pearson: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 10/98 [3:19:04<27:17:59, 1116.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 13/100, Loss: 0.6442\n",
      "Checkpoint saved for fold 1 at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 11/98 [3:25:28<27:02:41, 1119.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 13/100 - MSE: 0.9777, CI: 0.7384, Pearson: 0.6173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 11/98 [3:37:17<27:02:41, 1119.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 14/100, Loss: 0.6180\n",
      "Checkpoint saved for fold 1 at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 12/98 [3:43:39<26:31:54, 1110.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 14/100 - MSE: 0.8464, CI: 0.7457, Pearson: 0.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 12/98 [3:55:32<26:31:54, 1110.63s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 15/100, Loss: 0.6089\n",
      "Checkpoint saved for fold 1 at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 13/98 [4:01:55<26:06:51, 1106.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 15/100 - MSE: 0.8935, CI: 0.7492, Pearson: 0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 13/98 [4:13:42<26:06:51, 1106.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 16/100, Loss: 0.5886\n",
      "Checkpoint saved for fold 1 at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 14/98 [4:20:00<25:39:40, 1099.77s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 16/100 - MSE: 0.9251, CI: 0.7581, Pearson: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 14/98 [4:31:47<25:39:40, 1099.77s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 17/100, Loss: 0.5810\n",
      "Checkpoint saved for fold 1 at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▌        | 15/98 [4:38:05<25:15:24, 1095.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 17/100 - MSE: 1.0028, CI: 0.7604, Pearson: 0.6714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▌        | 15/98 [4:49:46<25:15:24, 1095.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 18/100, Loss: 0.5519\n",
      "Checkpoint saved for fold 1 at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▋        | 16/98 [4:55:58<24:47:51, 1088.68s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 18/100 - MSE: 1.0256, CI: 0.7687, Pearson: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▋        | 16/98 [5:07:37<24:47:51, 1088.68s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 19/100, Loss: 0.5526\n",
      "Checkpoint saved for fold 1 at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 17/98 [5:13:54<24:24:22, 1084.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 19/100 - MSE: 0.9649, CI: 0.7674, Pearson: 0.6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 17/98 [5:25:33<24:24:22, 1084.73s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 20/100, Loss: 0.5419\n",
      "Checkpoint saved for fold 1 at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 18/98 [5:31:50<24:02:49, 1082.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 20/100 - MSE: 1.0366, CI: 0.7673, Pearson: 0.6935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 18/98 [5:43:30<24:02:49, 1082.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 21/100, Loss: 0.5357\n",
      "Checkpoint saved for fold 1 at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 19/98 [5:49:44<23:41:34, 1079.68s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 21/100 - MSE: 1.0556, CI: 0.7761, Pearson: 0.7028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 19/98 [6:01:30<23:41:34, 1079.68s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 22/100, Loss: 0.5291\n",
      "Checkpoint saved for fold 1 at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 20/98 [6:07:42<23:23:07, 1079.33s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 22/100 - MSE: 1.0538, CI: 0.7742, Pearson: 0.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 20/98 [6:19:58<23:23:07, 1079.33s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 23/100, Loss: 0.5139\n",
      "Checkpoint saved for fold 1 at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██▏       | 21/98 [6:26:24<23:21:20, 1091.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 23/100 - MSE: 1.0722, CI: 0.7811, Pearson: 0.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██▏       | 21/98 [6:38:44<23:21:20, 1091.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 24/100, Loss: 0.5202\n",
      "Checkpoint saved for fold 1 at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 22/98 [6:45:06<23:14:39, 1101.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 24/100 - MSE: 1.1145, CI: 0.7874, Pearson: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 22/98 [6:57:40<23:14:39, 1101.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 25/100, Loss: 0.5024\n",
      "Checkpoint saved for fold 1 at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 23/98 [7:04:04<23:10:16, 1112.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 25/100 - MSE: 1.0912, CI: 0.7777, Pearson: 0.7173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 23/98 [7:16:11<23:10:16, 1112.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 26/100, Loss: 0.4956\n",
      "Checkpoint saved for fold 1 at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 24/98 [7:22:23<22:46:54, 1108.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 26/100 - MSE: 1.0309, CI: 0.7724, Pearson: 0.7177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 24/98 [7:34:11<22:46:54, 1108.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 27/100, Loss: 0.4918\n",
      "Checkpoint saved for fold 1 at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 25/98 [7:40:26<22:18:51, 1100.44s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 27/100 - MSE: 1.0226, CI: 0.7747, Pearson: 0.7221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 25/98 [7:52:11<22:18:51, 1100.44s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 28/100, Loss: 0.4882\n",
      "Checkpoint saved for fold 1 at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 26/98 [7:58:23<21:52:19, 1093.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 28/100 - MSE: 1.1133, CI: 0.7795, Pearson: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 26/98 [8:10:12<21:52:19, 1093.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 29/100, Loss: 0.4754\n",
      "Checkpoint saved for fold 1 at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 27/98 [8:16:25<21:30:02, 1090.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 29/100 - MSE: 1.0451, CI: 0.7875, Pearson: 0.7294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 27/98 [8:28:09<21:30:02, 1090.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 30/100, Loss: 0.4785\n",
      "Checkpoint saved for fold 1 at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▊       | 28/98 [8:34:36<21:12:01, 1090.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 30/100 - MSE: 1.0663, CI: 0.7858, Pearson: 0.7343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▊       | 28/98 [8:51:49<22:09:33, 1139.63s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 289\u001b[0m\n\u001b[1;32m    286\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 215\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    213\u001b[0m output \u001b[38;5;241m=\u001b[39m model(mol_batch, pro_batch)\n\u001b[1;32m    214\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target)\n\u001b[0;32m--> 215\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    217\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 128  # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 128  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 100  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZebVy1uuThZI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "Loading checkpoint for fold 1 from prepared_samples/TrainingModel/model_fold1_epoch30.pt\n",
      "Resuming training from epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   0%|          | 0/90 [11:58<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 31/120, Loss: 0.4399\n",
      "Checkpoint saved for fold 1 at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/90 [18:14<27:04:14, 1095.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 31/120 - MSE: 1.0760, CI: 0.7888, Pearson: 0.7425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/90 [30:12<27:04:14, 1095.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 32/120, Loss: 0.4396\n",
      "Checkpoint saved for fold 1 at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/90 [36:27<26:43:45, 1093.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 32/120 - MSE: 1.0232, CI: 0.7918, Pearson: 0.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/90 [48:59<26:43:45, 1093.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 33/120, Loss: 0.4380\n",
      "Checkpoint saved for fold 1 at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/90 [55:31<26:59:21, 1116.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 33/120 - MSE: 1.0571, CI: 0.7964, Pearson: 0.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/90 [1:08:21<26:59:21, 1116.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 34/120, Loss: 0.4374\n",
      "Checkpoint saved for fold 1 at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 4/90 [1:14:53<27:05:53, 1134.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 34/120 - MSE: 1.1009, CI: 0.7982, Pearson: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 4/90 [1:27:55<27:05:53, 1134.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 35/120, Loss: 0.4323\n",
      "Checkpoint saved for fold 1 at epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 5/90 [1:34:41<27:14:25, 1153.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 35/120 - MSE: 1.1038, CI: 0.7969, Pearson: 0.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 5/90 [1:47:44<27:14:25, 1153.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 36/120, Loss: 0.4295\n",
      "Checkpoint saved for fold 1 at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 6/90 [1:54:38<27:16:05, 1168.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 36/120 - MSE: 1.0726, CI: 0.7988, Pearson: 0.7532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 6/90 [2:07:21<27:16:05, 1168.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 37/120, Loss: 0.4233\n",
      "Checkpoint saved for fold 1 at epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 7/90 [2:13:58<26:52:32, 1165.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 37/120 - MSE: 1.0449, CI: 0.8004, Pearson: 0.7579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 7/90 [2:26:51<26:52:32, 1165.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 38/120, Loss: 0.4190\n",
      "Checkpoint saved for fold 1 at epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 8/90 [2:33:22<26:32:24, 1165.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 38/120 - MSE: 1.1403, CI: 0.8003, Pearson: 0.7525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 8/90 [2:45:45<26:32:24, 1165.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 39/120, Loss: 0.4081\n",
      "Checkpoint saved for fold 1 at epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 9/90 [2:52:17<26:00:05, 1155.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 39/120 - MSE: 1.0947, CI: 0.8015, Pearson: 0.7604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 9/90 [3:04:42<26:00:05, 1155.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 40/120, Loss: 0.4034\n",
      "Checkpoint saved for fold 1 at epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 10/90 [3:11:05<25:29:40, 1147.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 40/120 - MSE: 1.1249, CI: 0.8037, Pearson: 0.7616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 10/90 [3:23:10<25:29:40, 1147.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 41/120, Loss: 0.4031\n",
      "Checkpoint saved for fold 1 at epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 11/90 [3:29:30<24:53:37, 1134.40s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 41/120 - MSE: 1.0975, CI: 0.8017, Pearson: 0.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 11/90 [3:41:35<24:53:37, 1134.40s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 42/120, Loss: 0.4000\n",
      "Checkpoint saved for fold 1 at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 12/90 [3:47:53<24:22:10, 1124.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 42/120 - MSE: 1.0834, CI: 0.8042, Pearson: 0.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 12/90 [3:59:55<24:22:10, 1124.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 43/120, Loss: 0.3943\n",
      "Checkpoint saved for fold 1 at epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 13/90 [4:06:17<23:55:12, 1118.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 43/120 - MSE: 1.0799, CI: 0.7974, Pearson: 0.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 13/90 [4:18:17<23:55:12, 1118.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 44/120, Loss: 0.3952\n",
      "Checkpoint saved for fold 1 at epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 14/90 [4:24:36<23:29:15, 1112.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 44/120 - MSE: 1.0724, CI: 0.8055, Pearson: 0.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 14/90 [4:36:38<23:29:15, 1112.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 45/120, Loss: 0.3890\n",
      "Checkpoint saved for fold 1 at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 15/90 [4:42:57<23:06:12, 1108.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 45/120 - MSE: 1.1344, CI: 0.8024, Pearson: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 15/90 [4:55:00<23:06:12, 1108.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 46/120, Loss: 0.3830\n",
      "Checkpoint saved for fold 1 at epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 16/90 [5:01:18<22:44:57, 1106.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 46/120 - MSE: 1.1139, CI: 0.8095, Pearson: 0.7722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 16/90 [5:13:21<22:44:57, 1106.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 47/120, Loss: 0.3792\n",
      "Checkpoint saved for fold 1 at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 17/90 [5:19:40<22:24:46, 1105.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 47/120 - MSE: 1.0907, CI: 0.8066, Pearson: 0.7762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 17/90 [5:32:36<22:24:46, 1105.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 48/120, Loss: 0.3750\n",
      "Checkpoint saved for fold 1 at epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 18/90 [5:39:21<22:33:35, 1128.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 48/120 - MSE: 1.0592, CI: 0.8051, Pearson: 0.7742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 18/90 [5:52:32<22:33:35, 1128.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 49/120, Loss: 0.3771\n",
      "Checkpoint saved for fold 1 at epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 19/90 [5:59:18<22:39:31, 1148.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 49/120 - MSE: 1.0624, CI: 0.8107, Pearson: 0.7761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 19/90 [6:12:41<22:39:31, 1148.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 50/120, Loss: 0.3728\n",
      "Checkpoint saved for fold 1 at epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 20/90 [6:19:26<22:40:50, 1166.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 50/120 - MSE: 1.0951, CI: 0.8121, Pearson: 0.7795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 20/90 [6:32:42<22:40:50, 1166.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 51/120, Loss: 0.3688\n",
      "Checkpoint saved for fold 1 at epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 21/90 [6:38:53<22:21:48, 1166.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 51/120 - MSE: 1.0294, CI: 0.8115, Pearson: 0.7814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 21/90 [6:50:43<22:21:48, 1166.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 52/120, Loss: 0.3633\n",
      "Checkpoint saved for fold 1 at epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 21/90 [6:54:49<22:42:59, 1185.21s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 289\u001b[0m\n\u001b[1;32m    286\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 268\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m    267\u001b[0m mse \u001b[38;5;241m=\u001b[39m get_mse(total_labels, total_preds)\n\u001b[0;32m--> 268\u001b[0m ci \u001b[38;5;241m=\u001b[39m \u001b[43mget_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m pearson \u001b[38;5;241m=\u001b[39m get_pearson(total_labels, total_preds)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 112\u001b[0m, in \u001b[0;36mget_ci\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true[i] \u001b[38;5;241m!=\u001b[39m y_true[j]:\n\u001b[1;32m    110\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (y_true[i] \u001b[38;5;241m<\u001b[39m y_true[j] \u001b[38;5;129;01mand\u001b[39;00m y_pred[i] \u001b[38;5;241m<\u001b[39m y_pred[j]) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m--> 112\u001b[0m        (y_true[i] \u001b[38;5;241m>\u001b[39m y_true[j] \u001b[38;5;129;01mand\u001b[39;00m \u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[1;32m    113\u001b[0m         c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m y_pred[i] \u001b[38;5;241m==\u001b[39m y_pred[j]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 200  # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 200  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 120  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81BYRH82SfMx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QIpYAHS8-noy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "Loading checkpoint for fold 1 from prepared_samples/TrainingModel/model_fold1_epoch52.pt\n",
      "Resuming training from epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   0%|          | 0/198 [13:06<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 53/250, Loss: 0.3572\n",
      "Checkpoint saved for fold 1 at epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/198 [19:33<64:12:53, 1173.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 53/250 - MSE: 1.0370, CI: 0.8181, Pearson: 0.7939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/198 [32:14<64:12:53, 1173.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 54/250, Loss: 0.3532\n",
      "Checkpoint saved for fold 1 at epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 2/198 [38:43<63:07:43, 1159.51s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 54/250 - MSE: 1.0438, CI: 0.8176, Pearson: 0.7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 2/198 [51:29<63:07:43, 1159.51s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 55/250, Loss: 0.3491\n",
      "Checkpoint saved for fold 1 at epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 3/198 [58:06<62:53:36, 1161.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 55/250 - MSE: 1.0644, CI: 0.8180, Pearson: 0.7924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 3/198 [1:11:04<62:53:36, 1161.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 56/250, Loss: 0.3451\n",
      "Checkpoint saved for fold 1 at epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 4/198 [1:17:30<62:37:53, 1162.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 56/250 - MSE: 1.1196, CI: 0.8232, Pearson: 0.7978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 4/198 [1:30:42<62:37:53, 1162.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 57/250, Loss: 0.3397\n",
      "Checkpoint saved for fold 1 at epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 5/198 [1:37:10<62:39:30, 1168.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 57/250 - MSE: 1.0449, CI: 0.8194, Pearson: 0.8009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 5/198 [1:49:59<62:39:30, 1168.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 58/250, Loss: 0.3379\n",
      "Checkpoint saved for fold 1 at epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 6/198 [1:56:24<62:04:21, 1163.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 58/250 - MSE: 1.1172, CI: 0.8235, Pearson: 0.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 6/198 [2:09:38<62:04:21, 1163.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 59/250, Loss: 0.3373\n",
      "Checkpoint saved for fold 1 at epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▎         | 7/198 [2:16:10<62:07:22, 1170.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 59/250 - MSE: 1.0967, CI: 0.8226, Pearson: 0.7967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▎         | 7/198 [2:29:23<62:07:22, 1170.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 60/250, Loss: 0.3357\n",
      "Checkpoint saved for fold 1 at epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 8/198 [2:35:49<61:56:19, 1173.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 60/250 - MSE: 1.1151, CI: 0.8251, Pearson: 0.8026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 8/198 [2:48:58<61:56:19, 1173.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 61/250, Loss: 0.3327\n",
      "Checkpoint saved for fold 1 at epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▍         | 9/198 [2:55:24<61:37:51, 1173.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 61/250 - MSE: 1.0891, CI: 0.8295, Pearson: 0.8087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▍         | 9/198 [3:08:48<61:37:51, 1173.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 62/250, Loss: 0.3296\n",
      "Checkpoint saved for fold 1 at epoch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 10/198 [3:14:55<61:15:59, 1173.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 62/250 - MSE: 1.1296, CI: 0.8314, Pearson: 0.8135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 10/198 [3:26:57<61:15:59, 1173.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 63/250, Loss: 0.3258\n",
      "Checkpoint saved for fold 1 at epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 11/198 [3:33:10<59:41:25, 1149.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 63/250 - MSE: 1.1304, CI: 0.8286, Pearson: 0.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 11/198 [3:45:27<59:41:25, 1149.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 64/250, Loss: 0.3241\n",
      "Checkpoint saved for fold 1 at epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 12/198 [3:51:44<58:49:42, 1138.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 64/250 - MSE: 1.0846, CI: 0.8297, Pearson: 0.8116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 12/198 [4:04:00<58:49:42, 1138.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 65/250, Loss: 0.3201\n",
      "Checkpoint saved for fold 1 at epoch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 13/198 [4:10:10<57:59:59, 1128.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 65/250 - MSE: 1.0599, CI: 0.8275, Pearson: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 13/198 [4:22:26<57:59:59, 1128.64s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 66/250, Loss: 0.3203\n",
      "Checkpoint saved for fold 1 at epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 14/198 [4:28:42<57:25:22, 1123.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 66/250 - MSE: 1.1327, CI: 0.8264, Pearson: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 14/198 [4:41:02<57:25:22, 1123.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 67/250, Loss: 0.3151\n",
      "Checkpoint saved for fold 1 at epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 15/198 [4:47:15<56:57:30, 1120.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 67/250 - MSE: 1.1074, CI: 0.8317, Pearson: 0.8174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 15/198 [4:59:34<56:57:30, 1120.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 68/250, Loss: 0.3145\n",
      "Checkpoint saved for fold 1 at epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 16/198 [5:05:46<56:29:51, 1117.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 68/250 - MSE: 1.1396, CI: 0.8306, Pearson: 0.8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 16/198 [5:18:17<56:29:51, 1117.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 69/250, Loss: 0.3115\n",
      "Checkpoint saved for fold 1 at epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▊         | 17/198 [5:24:29<56:15:57, 1119.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 69/250 - MSE: 1.1543, CI: 0.8312, Pearson: 0.8162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▊         | 17/198 [5:38:15<56:15:57, 1119.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 70/250, Loss: 0.3105\n",
      "Checkpoint saved for fold 1 at epoch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 18/198 [5:44:40<57:20:19, 1146.78s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 70/250 - MSE: 1.0938, CI: 0.8317, Pearson: 0.8201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 18/198 [5:58:05<57:20:19, 1146.78s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 71/250, Loss: 0.3070\n",
      "Checkpoint saved for fold 1 at epoch 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|▉         | 19/198 [6:04:23<57:33:39, 1157.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 71/250 - MSE: 1.1410, CI: 0.8368, Pearson: 0.8252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|▉         | 19/198 [6:17:30<57:33:39, 1157.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 72/250, Loss: 0.3102\n",
      "Checkpoint saved for fold 1 at epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 20/198 [6:23:38<57:11:58, 1156.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 72/250 - MSE: 1.1440, CI: 0.8354, Pearson: 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 20/198 [6:36:04<57:11:58, 1156.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 73/250, Loss: 0.3000\n",
      "Checkpoint saved for fold 1 at epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 21/198 [6:42:16<56:18:13, 1145.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 73/250 - MSE: 1.1317, CI: 0.8355, Pearson: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 21/198 [6:54:42<56:18:13, 1145.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 74/250, Loss: 0.2994\n",
      "Checkpoint saved for fold 1 at epoch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 22/198 [7:00:49<55:31:25, 1135.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 74/250 - MSE: 1.1484, CI: 0.8320, Pearson: 0.8172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 22/198 [7:13:09<55:31:25, 1135.72s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 75/250, Loss: 0.2958\n",
      "Checkpoint saved for fold 1 at epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 23/198 [7:19:26<54:56:10, 1130.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 75/250 - MSE: 1.1473, CI: 0.8350, Pearson: 0.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 23/198 [7:32:06<54:56:10, 1130.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 76/250, Loss: 0.2972\n",
      "Checkpoint saved for fold 1 at epoch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 24/198 [7:38:18<54:38:28, 1130.51s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 76/250 - MSE: 1.1520, CI: 0.8413, Pearson: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 24/198 [7:51:14<54:38:28, 1130.51s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 77/250, Loss: 0.2903\n",
      "Checkpoint saved for fold 1 at epoch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 25/198 [7:57:32<54:39:59, 1137.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 77/250 - MSE: 1.1161, CI: 0.8386, Pearson: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 25/198 [8:10:56<54:39:59, 1137.57s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 78/250, Loss: 0.2937\n",
      "Checkpoint saved for fold 1 at epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 26/198 [8:17:13<54:58:33, 1150.66s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 78/250 - MSE: 1.0964, CI: 0.8376, Pearson: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 26/198 [8:31:25<54:58:33, 1150.66s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 79/250, Loss: 0.2939\n",
      "Checkpoint saved for fold 1 at epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▎        | 27/198 [8:37:50<55:53:20, 1176.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 79/250 - MSE: 1.0846, CI: 0.8346, Pearson: 0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▎        | 27/198 [8:50:35<55:53:20, 1176.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 80/250, Loss: 0.2871\n",
      "Checkpoint saved for fold 1 at epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 28/198 [8:56:48<55:00:56, 1165.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 80/250 - MSE: 1.1807, CI: 0.8414, Pearson: 0.8283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 28/198 [9:09:14<55:00:56, 1165.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 81/250, Loss: 0.2853\n",
      "Checkpoint saved for fold 1 at epoch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▍        | 29/198 [9:15:27<54:02:18, 1151.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 81/250 - MSE: 1.0849, CI: 0.8404, Pearson: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▍        | 29/198 [9:27:51<54:02:18, 1151.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 82/250, Loss: 0.2872\n",
      "Checkpoint saved for fold 1 at epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▌        | 30/198 [9:34:01<53:11:59, 1140.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 82/250 - MSE: 1.1513, CI: 0.8385, Pearson: 0.8287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▌        | 30/198 [9:47:56<53:11:59, 1140.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 83/250, Loss: 0.2808\n",
      "Checkpoint saved for fold 1 at epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 31/198 [9:54:11<53:51:31, 1161.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 83/250 - MSE: 1.1494, CI: 0.8412, Pearson: 0.8242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 31/198 [10:07:17<53:51:31, 1161.03s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 84/250, Loss: 0.2806\n",
      "Checkpoint saved for fold 1 at epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 32/198 [10:13:38<53:37:05, 1162.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 84/250 - MSE: 1.1007, CI: 0.8343, Pearson: 0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 32/198 [10:26:48<53:37:05, 1162.80s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 85/250, Loss: 0.2810\n",
      "Checkpoint saved for fold 1 at epoch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 33/198 [10:33:07<53:22:21, 1164.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 85/250 - MSE: 1.1405, CI: 0.8406, Pearson: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 33/198 [10:46:16<53:22:21, 1164.49s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 86/250, Loss: 0.2790\n",
      "Checkpoint saved for fold 1 at epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 34/198 [10:52:36<53:07:15, 1166.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 86/250 - MSE: 1.1511, CI: 0.8413, Pearson: 0.8328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 34/198 [11:06:13<53:07:15, 1166.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 87/250, Loss: 0.2744\n",
      "Checkpoint saved for fold 1 at epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 35/198 [11:12:33<53:13:04, 1175.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 87/250 - MSE: 1.1459, CI: 0.8396, Pearson: 0.8310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 35/198 [11:25:58<53:13:04, 1175.37s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 88/250, Loss: 0.2745\n",
      "Checkpoint saved for fold 1 at epoch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 36/198 [11:32:21<53:03:52, 1179.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 88/250 - MSE: 1.1533, CI: 0.8449, Pearson: 0.8351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 36/198 [11:45:58<53:03:52, 1179.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 89/250, Loss: 0.2687\n",
      "Checkpoint saved for fold 1 at epoch 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▊        | 37/198 [11:52:19<52:59:10, 1184.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 89/250 - MSE: 1.1709, CI: 0.8438, Pearson: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▊        | 37/198 [12:05:01<52:59:10, 1184.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 90/250, Loss: 0.2696\n",
      "Checkpoint saved for fold 1 at epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 38/198 [12:11:13<51:58:25, 1169.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 90/250 - MSE: 1.1495, CI: 0.8411, Pearson: 0.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 38/198 [12:23:43<51:58:25, 1169.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 91/250, Loss: 0.2680\n",
      "Checkpoint saved for fold 1 at epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|█▉        | 39/198 [12:29:54<51:00:15, 1154.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 91/250 - MSE: 1.1742, CI: 0.8437, Pearson: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|█▉        | 39/198 [12:42:27<51:00:15, 1154.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 92/250, Loss: 0.2662\n",
      "Checkpoint saved for fold 1 at epoch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 40/198 [12:48:40<50:18:17, 1146.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 92/250 - MSE: 1.1266, CI: 0.8438, Pearson: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|██        | 40/198 [13:01:01<50:18:17, 1146.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 93/250, Loss: 0.2684\n",
      "Checkpoint saved for fold 1 at epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 41/198 [13:07:09<49:30:03, 1135.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 93/250 - MSE: 1.1219, CI: 0.8438, Pearson: 0.8382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 41/198 [13:19:30<49:30:03, 1135.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 94/250, Loss: 0.2609\n",
      "Checkpoint saved for fold 1 at epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 42/198 [13:25:43<48:54:53, 1128.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 94/250 - MSE: 1.1546, CI: 0.8419, Pearson: 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 42/198 [13:38:20<48:54:53, 1128.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 95/250, Loss: 0.2627\n",
      "Checkpoint saved for fold 1 at epoch 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 43/198 [13:44:31<48:35:06, 1128.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 95/250 - MSE: 1.1111, CI: 0.8431, Pearson: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 43/198 [13:57:16<48:35:06, 1128.43s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 96/250, Loss: 0.2617\n",
      "Checkpoint saved for fold 1 at epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 44/198 [14:03:26<48:21:46, 1130.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 96/250 - MSE: 1.1435, CI: 0.8448, Pearson: 0.8357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 44/198 [14:16:01<48:21:46, 1130.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 97/250, Loss: 0.2611\n",
      "Checkpoint saved for fold 1 at epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 45/198 [14:22:13<48:00:16, 1129.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 97/250 - MSE: 1.1358, CI: 0.8477, Pearson: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 45/198 [14:34:47<48:00:16, 1129.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 98/250, Loss: 0.2583\n",
      "Checkpoint saved for fold 1 at epoch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 46/198 [14:40:56<47:36:15, 1127.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 98/250 - MSE: 1.1795, CI: 0.8440, Pearson: 0.8419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 46/198 [14:53:35<47:36:15, 1127.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 99/250, Loss: 0.2563\n",
      "Checkpoint saved for fold 1 at epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▎       | 47/198 [14:59:46<47:19:15, 1128.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 99/250 - MSE: 1.1733, CI: 0.8481, Pearson: 0.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▎       | 47/198 [15:12:26<47:19:15, 1128.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 100/250, Loss: 0.2563\n",
      "Checkpoint saved for fold 1 at epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 48/198 [15:18:40<47:05:24, 1130.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 100/250 - MSE: 1.1841, CI: 0.8462, Pearson: 0.8401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 48/198 [15:31:20<47:05:24, 1130.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 101/250, Loss: 0.2523\n",
      "Checkpoint saved for fold 1 at epoch 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▍       | 49/198 [15:37:37<46:50:58, 1131.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 101/250 - MSE: 1.1490, CI: 0.8451, Pearson: 0.8388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▍       | 49/198 [15:50:19<46:50:58, 1131.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 102/250, Loss: 0.2516\n",
      "Checkpoint saved for fold 1 at epoch 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▌       | 50/198 [15:56:35<46:36:55, 1133.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 102/250 - MSE: 1.1484, CI: 0.8475, Pearson: 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▌       | 50/198 [16:09:20<46:36:55, 1133.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 103/250, Loss: 0.2506\n",
      "Checkpoint saved for fold 1 at epoch 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 51/198 [16:15:34<46:21:53, 1135.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 103/250 - MSE: 1.2122, CI: 0.8473, Pearson: 0.8424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 51/198 [16:28:34<46:21:53, 1135.47s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 104/250, Loss: 0.2497\n",
      "Checkpoint saved for fold 1 at epoch 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▋       | 52/198 [16:34:48<46:16:25, 1140.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 104/250 - MSE: 1.1699, CI: 0.8491, Pearson: 0.8468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▋       | 52/198 [16:47:46<46:16:25, 1140.99s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 105/250, Loss: 0.2490\n",
      "Checkpoint saved for fold 1 at epoch 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 53/198 [16:54:09<46:12:10, 1147.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 105/250 - MSE: 1.1608, CI: 0.8460, Pearson: 0.8406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 53/198 [17:08:10<46:12:10, 1147.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 106/250, Loss: 0.2441\n",
      "Checkpoint saved for fold 1 at epoch 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 54/198 [17:14:16<46:35:47, 1164.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 106/250 - MSE: 1.1547, CI: 0.8469, Pearson: 0.8440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 54/198 [17:26:45<46:35:47, 1164.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 107/250, Loss: 0.2447\n",
      "Checkpoint saved for fold 1 at epoch 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 55/198 [17:32:57<45:45:14, 1151.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 107/250 - MSE: 1.2118, CI: 0.8470, Pearson: 0.8457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 55/198 [17:45:24<45:45:14, 1151.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 108/250, Loss: 0.2435\n",
      "Checkpoint saved for fold 1 at epoch 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 56/198 [17:51:36<45:02:53, 1142.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 108/250 - MSE: 1.1822, CI: 0.8505, Pearson: 0.8471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 56/198 [18:03:56<45:02:53, 1142.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 109/250, Loss: 0.2427\n",
      "Checkpoint saved for fold 1 at epoch 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 57/198 [18:10:08<44:22:08, 1132.83s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 109/250 - MSE: 1.1499, CI: 0.8497, Pearson: 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 57/198 [18:22:50<44:22:08, 1132.83s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 110/250, Loss: 0.2404\n",
      "Checkpoint saved for fold 1 at epoch 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 58/198 [18:29:15<44:13:09, 1137.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 110/250 - MSE: 1.1955, CI: 0.8486, Pearson: 0.8461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 58/198 [18:42:38<44:13:09, 1137.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 111/250, Loss: 0.2387\n",
      "Checkpoint saved for fold 1 at epoch 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|██▉       | 59/198 [18:48:51<44:21:10, 1148.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 111/250 - MSE: 1.1563, CI: 0.8513, Pearson: 0.8508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|██▉       | 59/198 [19:02:11<44:21:10, 1148.71s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 112/250, Loss: 0.2384\n",
      "Checkpoint saved for fold 1 at epoch 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|███       | 60/198 [19:08:33<44:25:30, 1158.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 112/250 - MSE: 1.2318, CI: 0.8509, Pearson: 0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|███       | 60/198 [19:21:58<44:25:30, 1158.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 113/250, Loss: 0.2379\n",
      "Checkpoint saved for fold 1 at epoch 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███       | 61/198 [19:28:11<44:19:19, 1164.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 113/250 - MSE: 1.1926, CI: 0.8483, Pearson: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███       | 61/198 [19:40:30<44:19:19, 1164.67s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 114/250, Loss: 0.2340\n",
      "Checkpoint saved for fold 1 at epoch 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███▏      | 62/198 [19:46:35<43:18:18, 1146.31s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 114/250 - MSE: 1.2180, CI: 0.8504, Pearson: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███▏      | 62/198 [19:59:25<43:18:18, 1146.31s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 115/250, Loss: 0.2335\n",
      "Checkpoint saved for fold 1 at epoch 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 63/198 [20:05:40<42:58:10, 1145.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 115/250 - MSE: 1.1550, CI: 0.8499, Pearson: 0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 63/198 [20:18:13<42:58:10, 1145.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 116/250, Loss: 0.2336\n",
      "Checkpoint saved for fold 1 at epoch 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 64/198 [20:24:25<42:25:40, 1139.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 116/250 - MSE: 1.1639, CI: 0.8499, Pearson: 0.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 64/198 [20:36:49<42:25:40, 1139.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 117/250, Loss: 0.2323\n",
      "Checkpoint saved for fold 1 at epoch 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 65/198 [20:43:02<41:51:23, 1132.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 117/250 - MSE: 1.1554, CI: 0.8506, Pearson: 0.8486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 65/198 [20:55:40<41:51:23, 1132.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 118/250, Loss: 0.2348\n",
      "Checkpoint saved for fold 1 at epoch 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 66/198 [21:01:55<41:32:14, 1132.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 118/250 - MSE: 1.1762, CI: 0.8528, Pearson: 0.8476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 66/198 [21:14:46<41:32:14, 1132.84s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 119/250, Loss: 0.2289\n",
      "Checkpoint saved for fold 1 at epoch 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 67/198 [21:20:55<41:18:25, 1135.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 119/250 - MSE: 1.1469, CI: 0.8545, Pearson: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 67/198 [21:33:26<41:18:25, 1135.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 120/250, Loss: 0.2272\n",
      "Checkpoint saved for fold 1 at epoch 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 68/198 [21:39:38<40:51:22, 1131.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 120/250 - MSE: 1.2068, CI: 0.8534, Pearson: 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 68/198 [21:52:13<40:51:22, 1131.41s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 121/250, Loss: 0.2303\n",
      "Checkpoint saved for fold 1 at epoch 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  35%|███▍      | 69/198 [21:58:27<40:30:42, 1130.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 121/250 - MSE: 1.1715, CI: 0.8504, Pearson: 0.8474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  35%|███▍      | 69/198 [22:11:04<40:30:42, 1130.56s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 122/250, Loss: 0.2266\n",
      "Checkpoint saved for fold 1 at epoch 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  35%|███▌      | 70/198 [22:17:12<40:08:30, 1128.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 122/250 - MSE: 1.2005, CI: 0.8536, Pearson: 0.8493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  35%|███▌      | 70/198 [22:29:39<40:08:30, 1128.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 123/250, Loss: 0.2270\n",
      "Checkpoint saved for fold 1 at epoch 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  36%|███▌      | 71/198 [22:35:49<39:41:46, 1125.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 123/250 - MSE: 1.1446, CI: 0.8522, Pearson: 0.8484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  36%|███▌      | 71/198 [22:48:21<39:41:46, 1125.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 124/250, Loss: 0.2207\n",
      "Checkpoint saved for fold 1 at epoch 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  36%|███▋      | 72/198 [22:54:33<39:22:36, 1125.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 124/250 - MSE: 1.1818, CI: 0.8523, Pearson: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  36%|███▋      | 72/198 [23:07:17<39:22:36, 1125.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 125/250, Loss: 0.2221\n",
      "Checkpoint saved for fold 1 at epoch 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  37%|███▋      | 73/198 [23:13:28<39:09:52, 1127.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 125/250 - MSE: 1.1858, CI: 0.8532, Pearson: 0.8529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  37%|███▋      | 73/198 [23:26:21<39:09:52, 1127.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 126/250, Loss: 0.2191\n",
      "Checkpoint saved for fold 1 at epoch 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  37%|███▋      | 74/198 [23:32:31<39:00:42, 1132.60s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 126/250 - MSE: 1.1736, CI: 0.8547, Pearson: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  37%|███▋      | 74/198 [23:45:09<39:00:42, 1132.60s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 127/250, Loss: 0.2197\n",
      "Checkpoint saved for fold 1 at epoch 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  38%|███▊      | 75/198 [23:51:21<38:39:52, 1131.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 127/250 - MSE: 1.1997, CI: 0.8556, Pearson: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  38%|███▊      | 75/198 [24:03:54<38:39:52, 1131.65s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 128/250, Loss: 0.2189\n",
      "Checkpoint saved for fold 1 at epoch 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  38%|███▊      | 76/198 [24:10:07<38:17:35, 1129.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 128/250 - MSE: 1.1862, CI: 0.8558, Pearson: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  38%|███▊      | 76/198 [24:22:43<38:17:35, 1129.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 129/250, Loss: 0.2182\n",
      "Checkpoint saved for fold 1 at epoch 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  39%|███▉      | 77/198 [24:29:08<38:05:50, 1133.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 129/250 - MSE: 1.1684, CI: 0.8573, Pearson: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  39%|███▉      | 77/198 [24:42:27<38:05:50, 1133.48s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 130/250, Loss: 0.2160\n",
      "Checkpoint saved for fold 1 at epoch 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  39%|███▉      | 78/198 [24:48:39<38:09:09, 1144.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 130/250 - MSE: 1.1937, CI: 0.8518, Pearson: 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  39%|███▉      | 78/198 [25:01:47<38:09:09, 1144.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 131/250, Loss: 0.2174\n",
      "Checkpoint saved for fold 1 at epoch 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|███▉      | 79/198 [25:08:01<38:00:12, 1149.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 131/250 - MSE: 1.1955, CI: 0.8554, Pearson: 0.8547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|███▉      | 79/198 [25:20:47<38:00:12, 1149.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 132/250, Loss: 0.2147\n",
      "Checkpoint saved for fold 1 at epoch 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|████      | 80/198 [25:26:57<37:33:11, 1145.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 132/250 - MSE: 1.1935, CI: 0.8545, Pearson: 0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  40%|████      | 80/198 [25:39:54<37:33:11, 1145.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 133/250, Loss: 0.2171\n",
      "Checkpoint saved for fold 1 at epoch 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  41%|████      | 81/198 [25:46:44<37:38:13, 1158.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 133/250 - MSE: 1.1685, CI: 0.8558, Pearson: 0.8508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  41%|████      | 81/198 [25:59:49<37:38:13, 1158.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 134/250, Loss: 0.2096\n",
      "Checkpoint saved for fold 1 at epoch 134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  41%|████      | 81/198 [26:03:10<37:37:54, 1157.90s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 289\u001b[0m\n\u001b[1;32m    286\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 268\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m    267\u001b[0m mse \u001b[38;5;241m=\u001b[39m get_mse(total_labels, total_preds)\n\u001b[0;32m--> 268\u001b[0m ci \u001b[38;5;241m=\u001b[39m \u001b[43mget_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m pearson \u001b[38;5;241m=\u001b[39m get_pearson(total_labels, total_preds)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m, in \u001b[0;36mget_ci\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true[i] \u001b[38;5;241m!=\u001b[39m y_true[j]:\n\u001b[1;32m    110\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (y_true[i] \u001b[38;5;241m<\u001b[39m y_true[j] \u001b[38;5;129;01mand\u001b[39;00m y_pred[i] \u001b[38;5;241m<\u001b[39m y_pred[j]) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m--> 112\u001b[0m        (y_true[i] \u001b[38;5;241m>\u001b[39m y_true[j] \u001b[38;5;129;01mand\u001b[39;00m \u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[1;32m    113\u001b[0m         c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m y_pred[i] \u001b[38;5;241m==\u001b[39m y_pred[j]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 256 # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 256  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 250  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jujD6XAOtboL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "Loading checkpoint for fold 1 from prepared_samples/TrainingModel/model_fold1_epoch134.pt\n",
      "Resuming training from epoch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   0%|          | 0/116 [13:33<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 135/250, Loss: 0.2120\n",
      "Checkpoint saved for fold 1 at epoch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/116 [20:02<38:24:04, 1202.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 135/250 - MSE: 1.1602, CI: 0.8533, Pearson: 0.8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   1%|          | 1/116 [33:27<38:24:04, 1202.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 136/250, Loss: 0.2120\n",
      "Checkpoint saved for fold 1 at epoch 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/116 [39:45<37:42:40, 1190.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 136/250 - MSE: 1.1482, CI: 0.8550, Pearson: 0.8539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   2%|▏         | 2/116 [53:13<37:42:40, 1190.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 137/250, Loss: 0.2086\n",
      "Checkpoint saved for fold 1 at epoch 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/116 [59:29<37:17:36, 1188.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 137/250 - MSE: 1.1583, CI: 0.8573, Pearson: 0.8539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 3/116 [1:12:42<37:17:36, 1188.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 138/250, Loss: 0.2108\n",
      "Checkpoint saved for fold 1 at epoch 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 4/116 [1:18:55<36:41:11, 1179.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 138/250 - MSE: 1.1979, CI: 0.8581, Pearson: 0.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   3%|▎         | 4/116 [1:31:53<36:41:11, 1179.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 139/250, Loss: 0.2066\n",
      "Checkpoint saved for fold 1 at epoch 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 5/116 [1:38:10<36:05:20, 1170.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 139/250 - MSE: 1.1869, CI: 0.8572, Pearson: 0.8548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   4%|▍         | 5/116 [1:51:22<36:05:20, 1170.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 140/250, Loss: 0.2072\n",
      "Checkpoint saved for fold 1 at epoch 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 6/116 [1:57:45<35:48:26, 1171.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 140/250 - MSE: 1.1759, CI: 0.8585, Pearson: 0.8554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   5%|▌         | 6/116 [2:11:39<35:48:26, 1171.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 141/250, Loss: 0.2098\n",
      "Checkpoint saved for fold 1 at epoch 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 7/116 [2:18:03<35:56:46, 1187.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 141/250 - MSE: 1.1773, CI: 0.8583, Pearson: 0.8572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   6%|▌         | 7/116 [2:32:09<35:56:46, 1187.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 142/250, Loss: 0.2065\n",
      "Checkpoint saved for fold 1 at epoch 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 8/116 [2:38:40<36:05:03, 1202.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 142/250 - MSE: 1.1901, CI: 0.8552, Pearson: 0.8563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   7%|▋         | 8/116 [2:52:32<36:05:03, 1202.81s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 143/250, Loss: 0.2054\n",
      "Checkpoint saved for fold 1 at epoch 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 9/116 [2:58:53<35:51:11, 1206.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 143/250 - MSE: 1.1932, CI: 0.8556, Pearson: 0.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   8%|▊         | 9/116 [3:11:53<35:51:11, 1206.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 144/250, Loss: 0.2046\n",
      "Checkpoint saved for fold 1 at epoch 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▊         | 10/116 [3:17:58<34:57:12, 1187.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 144/250 - MSE: 1.2295, CI: 0.8577, Pearson: 0.8603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▊         | 10/116 [3:31:52<34:57:12, 1187.10s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 145/250, Loss: 0.2031\n",
      "Checkpoint saved for fold 1 at epoch 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 11/116 [3:38:12<34:51:59, 1195.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 145/250 - MSE: 1.2576, CI: 0.8547, Pearson: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:   9%|▉         | 11/116 [3:52:02<34:51:59, 1195.42s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 146/250, Loss: 0.2000\n",
      "Checkpoint saved for fold 1 at epoch 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 12/116 [3:58:24<34:40:52, 1200.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 146/250 - MSE: 1.1480, CI: 0.8580, Pearson: 0.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  10%|█         | 12/116 [4:11:55<34:40:52, 1200.50s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 147/250, Loss: 0.2007\n",
      "Checkpoint saved for fold 1 at epoch 147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 13/116 [4:18:18<34:17:29, 1198.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 147/250 - MSE: 1.1918, CI: 0.8577, Pearson: 0.8513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  11%|█         | 13/116 [4:31:16<34:17:29, 1198.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 148/250, Loss: 0.2017\n",
      "Checkpoint saved for fold 1 at epoch 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 14/116 [4:37:29<33:33:11, 1184.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 148/250 - MSE: 1.2058, CI: 0.8554, Pearson: 0.8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  12%|█▏        | 14/116 [4:49:53<33:33:11, 1184.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 149/250, Loss: 0.1988\n",
      "Checkpoint saved for fold 1 at epoch 149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 15/116 [4:56:05<32:38:37, 1163.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 149/250 - MSE: 1.2168, CI: 0.8583, Pearson: 0.8591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  13%|█▎        | 15/116 [5:08:40<32:38:37, 1163.54s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 150/250, Loss: 0.1993\n",
      "Checkpoint saved for fold 1 at epoch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 16/116 [5:14:46<31:58:08, 1150.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 150/250 - MSE: 1.2167, CI: 0.8583, Pearson: 0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  14%|█▍        | 16/116 [5:27:06<31:58:08, 1150.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 151/250, Loss: 0.1998\n",
      "Checkpoint saved for fold 1 at epoch 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▍        | 17/116 [5:33:13<31:17:00, 1137.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 151/250 - MSE: 1.1708, CI: 0.8579, Pearson: 0.8534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  15%|█▍        | 17/116 [5:45:54<31:17:00, 1137.58s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 152/250, Loss: 0.2011\n",
      "Checkpoint saved for fold 1 at epoch 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 18/116 [5:52:20<31:02:28, 1140.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 152/250 - MSE: 1.2071, CI: 0.8562, Pearson: 0.8553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▌        | 18/116 [6:05:49<31:02:28, 1140.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 153/250, Loss: 0.1969\n",
      "Checkpoint saved for fold 1 at epoch 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▋        | 19/116 [6:12:13<31:09:17, 1156.27s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 153/250 - MSE: 1.2210, CI: 0.8577, Pearson: 0.8570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  16%|█▋        | 19/116 [6:25:37<31:09:17, 1156.27s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 154/250, Loss: 0.1955\n",
      "Checkpoint saved for fold 1 at epoch 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 20/116 [6:31:41<30:55:49, 1159.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 154/250 - MSE: 1.2090, CI: 0.8576, Pearson: 0.8570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  17%|█▋        | 20/116 [6:43:56<30:55:49, 1159.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 155/250, Loss: 0.1965\n",
      "Checkpoint saved for fold 1 at epoch 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 21/116 [6:50:07<30:10:50, 1143.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 155/250 - MSE: 1.2240, CI: 0.8596, Pearson: 0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  18%|█▊        | 21/116 [7:02:33<30:10:50, 1143.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 156/250, Loss: 0.1918\n",
      "Checkpoint saved for fold 1 at epoch 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 22/116 [7:08:38<29:36:06, 1133.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 156/250 - MSE: 1.2119, CI: 0.8578, Pearson: 0.8547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  19%|█▉        | 22/116 [7:21:03<29:36:06, 1133.69s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 157/250, Loss: 0.1926\n",
      "Checkpoint saved for fold 1 at epoch 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|█▉        | 23/116 [7:27:09<29:06:42, 1126.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 157/250 - MSE: 1.2402, CI: 0.8578, Pearson: 0.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  20%|█▉        | 23/116 [7:39:31<29:06:42, 1126.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 158/250, Loss: 0.1888\n",
      "Checkpoint saved for fold 1 at epoch 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 24/116 [7:45:40<28:40:54, 1122.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 158/250 - MSE: 1.1576, CI: 0.8608, Pearson: 0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  21%|██        | 24/116 [7:58:08<28:40:54, 1122.34s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 159/250, Loss: 0.1898\n",
      "Checkpoint saved for fold 1 at epoch 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 25/116 [8:04:14<28:18:21, 1119.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 159/250 - MSE: 1.2068, CI: 0.8578, Pearson: 0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 25/116 [8:16:44<28:18:21, 1119.79s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 160/250, Loss: 0.1888\n",
      "Checkpoint saved for fold 1 at epoch 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 26/116 [8:22:56<28:00:40, 1120.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 160/250 - MSE: 1.1977, CI: 0.8601, Pearson: 0.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  22%|██▏       | 26/116 [8:35:23<28:00:40, 1120.45s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 161/250, Loss: 0.1910\n",
      "Checkpoint saved for fold 1 at epoch 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 27/116 [8:41:31<27:39:16, 1118.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 161/250 - MSE: 1.1951, CI: 0.8622, Pearson: 0.8627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  23%|██▎       | 27/116 [8:54:05<27:39:16, 1118.62s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 162/250, Loss: 0.1888\n",
      "Checkpoint saved for fold 1 at epoch 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 28/116 [9:00:17<27:24:13, 1121.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 162/250 - MSE: 1.1946, CI: 0.8620, Pearson: 0.8626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  24%|██▍       | 28/116 [9:13:09<27:24:13, 1121.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 163/250, Loss: 0.1879\n",
      "Checkpoint saved for fold 1 at epoch 163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▌       | 29/116 [9:19:26<27:17:27, 1129.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 163/250 - MSE: 1.2116, CI: 0.8603, Pearson: 0.8574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  25%|██▌       | 29/116 [9:33:36<27:17:27, 1129.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 164/250, Loss: 0.1898\n",
      "Checkpoint saved for fold 1 at epoch 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 30/116 [9:39:50<27:39:26, 1157.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 164/250 - MSE: 1.2335, CI: 0.8618, Pearson: 0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  26%|██▌       | 30/116 [9:54:03<27:39:26, 1157.75s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 165/250, Loss: 0.1869\n",
      "Checkpoint saved for fold 1 at epoch 165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 31/116 [10:00:30<27:55:07, 1182.44s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 165/250 - MSE: 1.1986, CI: 0.8637, Pearson: 0.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  27%|██▋       | 31/116 [10:13:46<27:55:07, 1182.44s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 166/250, Loss: 0.1833\n",
      "Checkpoint saved for fold 1 at epoch 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 32/116 [10:19:57<27:28:51, 1177.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 166/250 - MSE: 1.1585, CI: 0.8632, Pearson: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 32/116 [10:32:23<27:28:51, 1177.76s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 167/250, Loss: 0.1842\n",
      "Checkpoint saved for fold 1 at epoch 167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 33/116 [10:38:27<26:41:13, 1157.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 167/250 - MSE: 1.1861, CI: 0.8614, Pearson: 0.8607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  28%|██▊       | 33/116 [10:50:50<26:41:13, 1157.52s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 168/250, Loss: 0.1858\n",
      "Checkpoint saved for fold 1 at epoch 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 34/116 [10:56:58<26:02:37, 1143.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 168/250 - MSE: 1.1994, CI: 0.8617, Pearson: 0.8616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  29%|██▉       | 34/116 [11:09:23<26:02:37, 1143.39s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 169/250, Loss: 0.1851\n",
      "Checkpoint saved for fold 1 at epoch 169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|███       | 35/116 [11:15:30<25:30:54, 1134.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 169/250 - MSE: 1.1956, CI: 0.8618, Pearson: 0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  30%|███       | 35/116 [11:27:54<25:30:54, 1134.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 170/250, Loss: 0.1857\n",
      "Checkpoint saved for fold 1 at epoch 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███       | 36/116 [11:34:01<25:02:52, 1127.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 170/250 - MSE: 1.2293, CI: 0.8613, Pearson: 0.8589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  31%|███       | 36/116 [11:46:59<25:02:52, 1127.15s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 171/250, Loss: 0.1848\n",
      "Checkpoint saved for fold 1 at epoch 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 37/116 [11:53:10<24:52:53, 1133.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 171/250 - MSE: 1.1696, CI: 0.8594, Pearson: 0.8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  32%|███▏      | 37/116 [12:06:00<24:52:53, 1133.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 172/250, Loss: 0.1814\n",
      "Checkpoint saved for fold 1 at epoch 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 38/116 [12:12:12<24:37:08, 1136.26s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 172/250 - MSE: 1.1879, CI: 0.8591, Pearson: 0.8605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  33%|███▎      | 38/116 [12:24:55<24:37:08, 1136.26s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 173/250, Loss: 0.1799\n",
      "Checkpoint saved for fold 1 at epoch 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▎      | 39/116 [12:31:08<24:17:53, 1136.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 173/250 - MSE: 1.1888, CI: 0.8608, Pearson: 0.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▎      | 39/116 [12:43:54<24:17:53, 1136.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 174/250, Loss: 0.1788\n",
      "Checkpoint saved for fold 1 at epoch 174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 40/116 [12:50:11<24:01:42, 1138.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 174/250 - MSE: 1.1817, CI: 0.8635, Pearson: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Fold 1:  34%|███▍      | 40/116 [12:58:53<24:39:53, 1168.34s/epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 289\u001b[0m\n\u001b[1;32m    286\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 215\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    213\u001b[0m output \u001b[38;5;241m=\u001b[39m model(mol_batch, pro_batch)\n\u001b[1;32m    214\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), target)\n\u001b[0;32m--> 215\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    217\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true ) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 256 # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "            avg_loss = running_loss / len(train_files)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 256  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 250  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nSEjnamUj59k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu.\n",
      "Using existing TrainingModel directory at prepared_samples/TrainingModel\n",
      "\n",
      "Fold 1/5\n",
      "GNNNet Loaded\n",
      "Model is on device: cpu\n",
      "Loading checkpoint for fold 1 from prepared_samples/TrainingModel/model_fold1_epoch249.pt\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 289\u001b[0m\n\u001b[1;32m    286\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Run the training function\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_5fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Print overall results\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCross-validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 176\u001b[0m, in \u001b[0;36mtrain_5fold_cross_validation\u001b[0;34m(sample_dir, num_epochs, n_splits, lr)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint for fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 176\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    177\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    178\u001b[0m loaded_epoch \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "# Suppress FutureWarning related to torch.load\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define the load_sample function\n",
    "def load_sample(path):\n",
    "    # Load individual sample from file\n",
    "    sample = torch.load(path)\n",
    "    mol_data = sample[0]\n",
    "    pro_data = sample[1]\n",
    "    target = sample[2]\n",
    "\n",
    "    # Convert dictionaries to Data objects if necessary\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Ensure that 'x' attribute is set\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data does not have 'x' or 'features' attribute\")\n",
    "\n",
    "    # Ensure 'x' is a float tensor\n",
    "    if not isinstance(mol_data.x, torch.Tensor):\n",
    "        mol_data.x = torch.tensor(mol_data.x)\n",
    "    if not isinstance(pro_data.x, torch.Tensor):\n",
    "        pro_data.x = torch.tensor(pro_data.x)\n",
    "\n",
    "    if mol_data.x.dtype != torch.float:\n",
    "        mol_data.x = mol_data.x.float()\n",
    "    if pro_data.x.dtype != torch.float:\n",
    "        pro_data.x = pro_data.x.float()\n",
    "\n",
    "    # Adjust 'edge_index' for mol_data\n",
    "    # Ensure 'edge_index' is a tensor of type torch.long\n",
    "    if not isinstance(mol_data.edge_index, torch.Tensor):\n",
    "        mol_data.edge_index = torch.tensor(mol_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        mol_data.edge_index = mol_data.edge_index.long()\n",
    "\n",
    "    # Ensure 'edge_index' has shape [2, num_edges]\n",
    "    if mol_data.edge_index.shape[0] != 2:\n",
    "        mol_data.edge_index = mol_data.edge_index.t()\n",
    "\n",
    "    # Adjust 'edge_index' for pro_data\n",
    "    if not isinstance(pro_data.edge_index, torch.Tensor):\n",
    "        pro_data.edge_index = torch.tensor(pro_data.edge_index, dtype=torch.long)\n",
    "    else:\n",
    "        pro_data.edge_index = pro_data.edge_index.long()\n",
    "\n",
    "    if pro_data.edge_index.shape[0] != 2:\n",
    "        pro_data.edge_index = pro_data.edge_index.t()\n",
    "\n",
    "    # Set 'num_nodes' attribute to suppress warnings\n",
    "    mol_data.num_nodes = mol_data.x.size(0)\n",
    "    pro_data.num_nodes = pro_data.x.size(0)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "# Define the batch_loader function\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    batch = []\n",
    "    for idx, file_name in enumerate(file_list):\n",
    "        sample_path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(sample_path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "\n",
    "# Define the evaluation metrics functions\n",
    "def get_mse(y_true, y_pred):\n",
    "    return np.mean((y_pred - y_true ) ** 2)\n",
    "\n",
    "def get_ci(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index between true and predicted values.\n",
    "    \"\"\"\n",
    "    pairs = itertools.combinations(range(len(y_true)), 2)\n",
    "    c = 0\n",
    "    s = 0\n",
    "    for i, j in pairs:\n",
    "        if y_true[i] != y_true[j]:\n",
    "            s += 1\n",
    "            if (y_true[i] < y_true[j] and y_pred[i] < y_pred[j]) or \\\n",
    "               (y_true[i] > y_true[j] and y_pred[i] > y_pred[j]):\n",
    "                c += 1\n",
    "            elif y_pred[i] == y_pred[j]:\n",
    "                c += 0.5\n",
    "    return c / s if s != 0 else 0\n",
    "\n",
    "def get_pearson(y_true, y_pred):\n",
    "    return pearsonr(y_true.flatten(), y_pred.flatten())[0]\n",
    "\n",
    "def train_5fold_cross_validation(sample_dir, num_epochs=1000, n_splits=5, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a single directory for all checkpoints\n",
    "    training_model_dir = os.path.join(sample_dir, 'TrainingModel')\n",
    "    if not os.path.exists(training_model_dir):\n",
    "        os.makedirs(training_model_dir)\n",
    "        print(f\"Created directory for checkpoints at {training_model_dir}\")\n",
    "    else:\n",
    "        print(f\"Using existing TrainingModel directory at {training_model_dir}\")\n",
    "\n",
    "    results = []\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(sample_files)):\n",
    "        fold_number = fold + 1\n",
    "        print(f'\\nFold {fold_number}/{n_splits}')\n",
    "        train_files = [sample_files[i] for i in train_idx]\n",
    "        test_files = [sample_files[i] for i in test_idx]\n",
    "\n",
    "        # Determine input feature dimensions from your data\n",
    "        sample = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "        mol_data = sample[0]\n",
    "        pro_data = sample[1]\n",
    "\n",
    "        num_features_mol = mol_data.x.size(1)\n",
    "        num_features_pro = pro_data.x.size(1)\n",
    "\n",
    "        # Initialize the GNN model with correct input dimensions\n",
    "        model = GNNNet(\n",
    "            num_features_mol=num_features_mol,\n",
    "            num_features_pro=num_features_pro\n",
    "        ).to(device)\n",
    "        print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Initialize starting epoch\n",
    "        start_epoch = 1\n",
    "\n",
    "        # Check for existing checkpoints in TrainingModel directory for the current fold\n",
    "        existing_checkpoints = [f for f in os.listdir(training_model_dir)\n",
    "                                if f.endswith('.pt') and f.startswith(f'model_fold{fold_number}_epoch')]\n",
    "\n",
    "        if existing_checkpoints:\n",
    "            # Find the latest checkpoint based on epoch number\n",
    "            latest_checkpoint = max(existing_checkpoints,\n",
    "                                    key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "            checkpoint_path = os.path.join(training_model_dir, latest_checkpoint)\n",
    "            print(f\"Loading checkpoint for fold {fold_number} from {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            loaded_epoch = checkpoint['epoch']\n",
    "            start_epoch = loaded_epoch + 1\n",
    "            print(f\"Resuming training from epoch {start_epoch}\")\n",
    "        else:\n",
    "            print(f\"No checkpoint found for fold {fold_number}, starting training from scratch.\")\n",
    "\n",
    "        # Training loop with progress bar over epochs\n",
    "        for epoch in tqdm(range(start_epoch, num_epochs + 1),\n",
    "                          desc=f\"Training Fold {fold_number}\", unit=\"epoch\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Prepare batch loader without progress bar for batches\n",
    "            batch_size = 256 # Adjust batch size as needed\n",
    "            batch_loader_iter = batch_loader(train_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "            for batch_samples in batch_loader_iter:\n",
    "                mol_data_list = []\n",
    "                pro_data_list = []\n",
    "                target_list = []\n",
    "\n",
    "                for sample in batch_samples:\n",
    "                    mol_data = sample[0]\n",
    "                    pro_data = sample[1]\n",
    "                    target = sample[2]\n",
    "\n",
    "                    mol_data_list.append(mol_data)\n",
    "                    pro_data_list.append(pro_data)\n",
    "                    target_list.append(target)\n",
    "\n",
    "                mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(mol_batch, pro_batch)\n",
    "                loss = loss_fn(output.view(-1), target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(batch_samples)\n",
    "            # Use tqdm.write() to print without interfering with the progress bar\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save the model and optimizer states after each epoch\n",
    "            checkpoint_filename = f\"model_fold{fold_number}_epoch{epoch}.pt\"\n",
    "            checkpoint_path = os.path.join(training_model_dir, checkpoint_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            tqdm.write(f\"Checkpoint saved for fold {fold_number} at epoch {epoch}\")\n",
    "\n",
    "            # Evaluation on the test set after each epoch\n",
    "            model.eval()\n",
    "            total_preds, total_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                batch_size = 256  # Adjust batch size as needed\n",
    "                batch_loader_iter = batch_loader(test_files, sample_dir, batch_size=batch_size)\n",
    "\n",
    "                for batch_samples in batch_loader_iter:\n",
    "                    mol_data_list = []\n",
    "                    pro_data_list = []\n",
    "                    target_list = []\n",
    "\n",
    "                    for sample in batch_samples:\n",
    "                        mol_data = sample[0]\n",
    "                        pro_data = sample[1]\n",
    "                        target = sample[2]\n",
    "\n",
    "                        mol_data_list.append(mol_data)\n",
    "                        pro_data_list.append(pro_data)\n",
    "                        target_list.append(target)\n",
    "\n",
    "                    mol_batch = Batch.from_data_list(mol_data_list).to(device)\n",
    "                    pro_batch = Batch.from_data_list(pro_data_list).to(device)\n",
    "                    target = torch.tensor(target_list, dtype=torch.float32).view(-1).to(device)\n",
    "\n",
    "                    output = model(mol_batch, pro_batch)\n",
    "                    total_preds.append(output.cpu().numpy())\n",
    "                    total_labels.append(target.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for evaluation\n",
    "            total_preds = np.concatenate(total_preds)\n",
    "            total_labels = np.concatenate(total_labels)\n",
    "\n",
    "            # Calculate metrics\n",
    "            mse = get_mse(total_labels, total_preds)\n",
    "            ci = get_ci(total_labels, total_preds)\n",
    "            pearson = get_pearson(total_labels, total_preds)\n",
    "\n",
    "            # Print metrics\n",
    "            tqdm.write(f\"Fold {fold_number}, Epoch {epoch}/{num_epochs} - MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "\n",
    "        # Evaluation at the end of training for this fold\n",
    "        print(f\"Final evaluation for Fold {fold_number}: MSE: {mse:.4f}, CI: {ci:.4f}, Pearson: {pearson:.4f}\")\n",
    "        # Store results for this fold\n",
    "        results.append((mse, ci, pearson))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_dir = 'prepared_samples'  # Adjust the path to your samples directory\n",
    "    num_epochs = 250  # Adjust the number of epochs as needed\n",
    "    n_splits = 5  # Number of folds for cross-validation\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Run the training function\n",
    "    results = train_5fold_cross_validation(sample_dir, num_epochs=num_epochs, n_splits=n_splits, lr=learning_rate)\n",
    "\n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation Results:\")\n",
    "    for fold_idx, (mse, ci, pearson) in enumerate(results):\n",
    "        print(f\"Fold {fold_idx + 1}: MSE={mse:.4f}, CI={ci:.4f}, Pearson={pearson:.4f}\")\n",
    "\n",
    "    # Optionally, compute and print average metrics across folds\n",
    "    mse_values, ci_values, pearson_values = zip(*results)\n",
    "    print(f\"\\nAverage Results:\")\n",
    "    print(f\"MSE: {np.mean(mse_values):.4f}\")\n",
    "    print(f\"CI: {np.mean(ci_values):.4f}\")\n",
    "    print(f\"Pearson Correlation: {np.mean(pearson_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKumGOKr81Bc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNtWlmw7DJK5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXuV9FqHxmw6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHKQ694Sm5r8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUWXUGXund5w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
